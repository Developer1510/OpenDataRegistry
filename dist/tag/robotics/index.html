<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
  
    
  
    <link rel="alternate" type="application/rss+xml" href="/rss.xml" />
    
    <link rel="shortcut icon" type="image/ico" href="https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico"/>
    <link rel="apple-touch-icon" sizes="57x57" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="72x72" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>
    <link rel="apple-touch-icon" sizes="114x114" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="144x144" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>  
  
    <title>Registry of Open Data on AWS</title>
  
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://assets.opendata.aws/css/bootstrap/3.4.1/bootstrap.min.css">
  
    <!-- Our local CSS -->
    <link rel="stylesheet" href="/css/main.css">
  
  
  </head>
  <body>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://assets.opendata.aws/js/jquery/3.5.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://assets.opendata.aws/js/bootstrap/3.4.1/bootstrap.min.js"></script>
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="roda-header">
          <h2><a href="/" alt="Home">Registry of Open Data on AWS</a></h2>
          <a href="http://aws.amazon.com/what-is-cloud-computing">
            <img src="https://assets.opendata.aws/img/AWS-Logo_White-Color_300x180.png" alt="Powered by AWS Cloud Computing" id="aws_header_logo">
          </a>
        </div>
      </div>
    </nav>
    <div class="container" >

    <div class="about col-md-5">
      <div class="row aboutbox">
            <h3>About</h3>
            <p>This registry exists to help people discover and share datasets that are available via AWS resources. <a href="https://opendata.aws">Learn more about sharing data on AWS</a>.</p>
    
            <p>See <a href="/tag/robotics/usage-examples">all usage examples for datasets listed in this registry</a> tagged with <strong>robotics</strong>.</p>
    
        <hr>
    
        <h4>Search datasets (currently <span id="count-matching">13</span> matching <span id="count-matching-text">datasets</span>)</h4>
        <form>
          <div class="form-group">
            <input type="text" id="search-box" class="form-control" placeholder="Search datasets" spellcheck="false" autocorrect="off">
          </div>
        </form>
        <p>You are currently viewing a subset of data tagged with <strong>robotics</strong>.</p>
    
        <hr>
    
        <h4>Add to this registry</h4>
        <p>If you want to add a dataset or example of how to use a dataset to this registry, please follow the instructions on the <a href="https://github.com/awslabs/open-data-registry/">Registry of Open Data on AWS GitHub repository</a>.</p>
    
        <p>Unless specifically stated in the applicable dataset documentation, datasets available through the Registry of Open Data on AWS are not provided and maintained by AWS. Datasets are provided and maintained by a variety of third parties under a variety of licenses. Please check dataset licenses and related documentation to determine if a dataset may be used for your application.</p>
      </div>
    </div>
    <div class="col-md-6 col-md-offset-1 datasets">
      <div class="row">
        <div id="ycb-benchmarks" class="dataset">
          <h3><a href="/ycb-benchmarks/">Yale-CMU-Berkeley (YCB) Object and Model Set</a></h3>
          <p><span class="label label-info tag link-tag">robotics</span></p>
          <p>This project primarily aims to facilitate performance benchmarking in robotics research. The dataset provides mesh models, RGB, RGB-D and point cloud images of over 80 objects. The physical objects are also available via the <a href="http://www.ycbbenchmarks.com/">YCB benchmarking project</a>. The data are collected by two state of the art systems: UC Berkley&#39;s scanning rig and the Google scanner. The UC Berkley&#39;s scanning rig data provide meshes generated with Poisson reconstruction, meshes generated with volumetric range image integration, textured versions of both meshes, Kinbody files for using the meshes with OpenRAVE, 600 ...</p>
          <p><a href="/ycb-benchmarks/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://ieeexplore.ieee.org/abstract/document/7989594" class="" target="_none">Pre-touch sensing for sequential manipulation</a> by <a href="https://sensor.cs.washington.edu/jrs.html" target="_none">Boling Yang, Patrick Lancaster, Joshua R. Smith</a></li></a>
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/abstract/document/8460950" class="" target="_none">Label Fusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes</a> by <a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_none">Pat Marion, Peter R. Florence, Lucas Manuelli, Russ Tedrake</a></li></a>
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/abstract/document/8304743" class="" target="_none">The Closure Signature: A Functional Approach to Model Underactuated Compliant Robotic Hands</a> by <a href="http://www.dii.unisi.it/~domenico/" target="_none">Maria Pozzi, Gionata Salvietti, Jo√£o Bimbo, Monica Malvezzi, Domenico Prattichizzo</a></li></a>
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/document/7254318" class="" target="_none">Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set</a> by <a href="https://www.wpi.edu/people/faculty/bcalli" target="_none">Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, Pieter Abbeel, Aaron M Dollar</a></li></a>
          </li>
        </ul>
        <p><a href="/ycb-benchmarks/#usageexamples">See 4 usage examples &rarr;</a></p>
        </div>
        <div id="ford-multi-av-seasonal" class="dataset">
          <h3><a href="/ford-multi-av-seasonal/">Ford Multi-AV Seasonal Dataset</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">lidar</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">robotics</span><span class="label label-info tag link-tag">transportation</span><span class="label label-info tag link-tag">urban</span><span class="label label-info tag link-tag">weather</span></p>
          <p>This research presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. The vehicles The vehicles were manually driven on an average route of 66 km in Michigan that included a mix of driving scenarios like the Detroit Airport, freeways, city-centres, university campus and suburban neighbourhood, etc. Each vehicle used in this data collection  is a Ford Fusion outfitted with an Applanix POS-LV inertial measurement unit (IMU), four HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged on the...</p>
          <p><a href="/ford-multi-av-seasonal/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/Ford/AVData" class="" target="_none">Ford AV Dataset Tutorial</a> by Ford Motor Company</li></a>
          </li>
        </ul>
        <p><a href="/ford-multi-av-seasonal/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="aev-a2d2" class="dataset">
          <h3><a href="/aev-a2d2/">A2D2: Audi Autonomous Driving Dataset</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">lidar</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">robotics</span></p>
          <p>An open multi-sensor dataset for autonomous driving research. This dataset comprises semantically segmented images, semantic point clouds, and 3D bounding boxes. In addition, it contains unlabelled 360 degree camera images, lidar, and bus data for three sequences. We hope this dataset will further facilitate active research and development in AI, computer vision, and robotics for autonomous driving.</p>
          <p><a href="/aev-a2d2/">Details &rarr;</a></p>
        </div>
        <div id="kitti" class="dataset">
          <h3><a href="/kitti/">KITTI Vision Benchmark Suite</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">robotics</span></p>
          <p>Dataset and benchmarks for computer vision research in the context of autonomous driving. The dataset has been recorded in and around the city of Karlsruhe, Germany using the mobile platform AnnieWay (VW station wagon) which has been equipped with several RGB and monochrome cameras, a Velodyne HDL 64 laser scanner as well as an accurate RTK corrected GPS/IMU localization unit. The dataset has been created for computer vision and machine learning research on stereo, optical flow, visual odometry, semantic segmentation, semantic instance segmentation, road segmentation, single image depth predic...</p>
          <p><a href="/kitti/">Details &rarr;</a></p>
        </div>
      </div>
    </div>


    <hr/>
  </div>  </body>
  <script src="/js/index.js"></script>
</html>
