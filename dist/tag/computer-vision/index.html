<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
  
    
  
    <link rel="alternate" type="application/rss+xml" href="/rss.xml" />
    
    <link rel="shortcut icon" type="image/ico" href="https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico"/>
    <link rel="apple-touch-icon" sizes="57x57" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="72x72" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>
    <link rel="apple-touch-icon" sizes="114x114" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="144x144" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>  
  
    <title>Registry of Open Data on AWS</title>
  
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://assets.opendata.aws/css/bootstrap/3.4.1/bootstrap.min.css">
  
    <!-- Our local CSS -->
    <link rel="stylesheet" href="/css/main.css">
  
  
  </head>
  <body>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://assets.opendata.aws/js/jquery/3.5.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://assets.opendata.aws/js/bootstrap/3.4.1/bootstrap.min.js"></script>
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="roda-header">
          <h2><a href="/" alt="Home">Registry of Open Data on AWS</a></h2>
          <a href="http://aws.amazon.com/what-is-cloud-computing">
            <img src="https://assets.opendata.aws/img/AWS-Logo_White-Color_300x180.png" alt="Powered by AWS Cloud Computing" id="aws_header_logo">
          </a>
        </div>
      </div>
    </nav>
    <div class="container" >

    <div class="about col-md-5">
      <div class="row aboutbox">
            <h3>About</h3>
            <p>This registry exists to help people discover and share datasets that are available via AWS resources. <a href="https://opendata.aws">Learn more about sharing data on AWS</a>.</p>
    
            <p>See <a href="/tag/computer-vision/usage-examples">all usage examples for datasets listed in this registry</a> tagged with <strong>computer vision</strong>.</p>
    
        <hr>
    
        <h4>Search datasets (currently <span id="count-matching">13</span> matching <span id="count-matching-text">datasets</span>)</h4>
        <form>
          <div class="form-group">
            <input type="text" id="search-box" class="form-control" placeholder="Search datasets" spellcheck="false" autocorrect="off">
          </div>
        </form>
        <p>You are currently viewing a subset of data tagged with <strong>computer vision</strong>.</p>
    
        <hr>
    
        <h4>Add to this registry</h4>
        <p>If you want to add a dataset or example of how to use a dataset to this registry, please follow the instructions on the <a href="https://github.com/awslabs/open-data-registry/">Registry of Open Data on AWS GitHub repository</a>.</p>
    
        <p>Unless specifically stated in the applicable dataset documentation, datasets available through the Registry of Open Data on AWS are not provided and maintained by AWS. Datasets are provided and maintained by a variety of third parties under a variety of licenses. Please check dataset licenses and related documentation to determine if a dataset may be used for your application.</p>
      </div>
    </div>
    <div class="col-md-6 col-md-offset-1 datasets">
      <div class="row">
        <div id="spacenet" class="dataset">
          <h3><a href="/spacenet/">SpaceNet</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">satellite imagery</span></p>
          <p>SpaceNet, launched in August 2016 as an open innovation project offering a repository of freely available
imagery with co-registered map features. Before SpaceNet, computer vision researchers had minimal options
to obtain free, precision-labeled, and high-resolution satellite imagery. Today, SpaceNet hosts datasets
developed by its own team, along with data sets from projects like IARPA’s Functional Map of the World (fMoW).</p>
          <p><a href="/spacenet/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://medium.com/the-downlinq/spacenet-winning-implementations-and-new-imagery-release-55f738e14253" class="" target="_none">SpaceNet: Winning Implementations and New Imagery Release</a> by <a href="https://medium.com/@toddstavish" target="_none">Todd Stavish</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/creating-training-datasets-for-the-spacenet-road-detection-and-routing-challenge-6f970d413e2f" class="" target="_none">Creating Training Datasets for the SpaceNet Road Detection and Routing Challenge</a> by <a href="https://medium.com/@avanetten" target="_none">Adam Van Etten and Jake Shermeyer</a></li></a>
          </li>
          <li>
            <a href="https://aws.amazon.com/blogs/machine-learning/extracting-buildings-and-roads-from-aws-open-data-using-amazon-sagemaker/" class="aws-link" target="_none">Extracting buildings and roads from AWS Open Data using Amazon SageMaker</a> by Yunzhi Shi, Tianyu Zhang, and Xin Chen</li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/getting-started-with-spacenet-data-827fd2ec9f53" class="" target="_none">Getting Started with SpaceNet Data</a> by <a href="https://medium.com/@avanetten" target="_none">Adam Van Etten</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/spacenet-5-dataset-release-80bd82d7c528" class="" target="_none">SpaceNet 5 Dataset Release</a> by <a href="https://medium.com/@avanetten" target="_none">Adam Van Etten and Ryan Lewis</a></li></a>
          </li>
        </ul>
        <p><a href="/spacenet/#usageexamples">See 11 usage examples &rarr;</a></p>
        </div>
        <div id="ladi" class="dataset">
          <h3><a href="/ladi/">Low Altitude Disaster Imagery (LADI) Dataset</a></h3>
          <p><span class="label label-info tag link-tag">aerial imagery</span><span class="label label-info tag link-tag">coastal</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">earthquakes</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">infrastructure</span><span class="label label-info tag link-tag">land</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">natural resource</span><span class="label label-info tag link-tag">seismology</span><span class="label label-info tag link-tag">transportation</span><span class="label label-info tag link-tag">urban</span><span class="label label-info tag link-tag">water</span></p>
          <p>The Low Altitude Disaster Imagery (LADI) Dataset consists of human and machine annotated airborne images collected by the Civil Air Patrol in support of various disaster responses from 2015-2019. The initial release of LADI focuses on the Atlantic hurricane seasons and coastal states along the Atlantic Ocean and Gulf of Mexico. Annotations are included for major hurricanes of Harvey, Maria, and Florence. Two key distinctions are the low altitude, oblique perspective of the imagery and disaster-related features, which are rarely featured in computer vision benchmarks and datasets.</p>
          <p><a href="/ladi/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/2005.05495" class="" target="_none">Train and Deploy an Image Classifier for Disaster Response</a> by Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren</li></a>
          </li>
          <li>
            <a href="https://www-nlpir.nist.gov/projects/tv2020/dsdi.html" class="" target="_none">NIST TRECVID 2020 - Disaster Scene Description and Indexing (DSDI)</a> by TREC Video Retrieval Evaluation (TRECVID)</li></a>
          </li>
          <li>
            <a href="https://github.com/bwsi-hadr" class="" target="_none">Remote Sensing for Disaster Response Course</a> by <a href="https://beaverworks.ll.mit.edu/CMS/bw/bwsi" target="_none">Beaver Works Summer Institute</a></li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1109/LNET.2019.2955833" class="" target="_none">Video Testing at the FirstNet Innovation and Test Lab Using a Public Safety Dataset</a> by Chris Budny, Jeffrey Liu, Andrew Weinert</li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1109/HPEC.2019.8916437" class="" target="_none">Large Scale Organization and Inference of an Imagery Dataset for Public Safety</a> by Jeffrey Liu, David Strohschein, Siddharth Samsi, Andrew Weinert</li></a>
          </li>
        </ul>
        <p><a href="/ladi/#usageexamples">See 6 usage examples &rarr;</a></p>
        </div>
        <div id="amazon-bin-imagery" class="dataset">
          <h3><a href="/amazon-bin-imagery/">Amazon Bin Image Dataset</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>The Amazon Bin Image Dataset contains over 500,000 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations.</p>
          <p><a href="/amazon-bin-imagery/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/silverbottlep/abid_challenge" class="" target="_none">Amazon Bin Image Dataset Challenge</a> by <a href="https://github.com/silverbottlep" target="_none">silverbottlep</a></li></a>
          </li>
          <li>
            <a href="https://github.com/OneNow/AI-Inventory-Reconciliation" class="" target="_none">Amazon Inventory Reconciliation using AI</a> by <a href="https://github.com/pablo-tech" target="_none">Pablo Rodriguez Bertorello, Sravan Sripada, Nutchapol Dendumrongsup</a></li></a>
          </li>
        </ul>
        <p><a href="/amazon-bin-imagery/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="janelia-cosem" class="dataset">
          <h3><a href="/janelia-cosem/">Cell Organelle Segmentation in Electron Microscopy (COSEM) on AWS</a></h3>
          <p><span class="label label-info tag link-tag">cell biology</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">electron microscopy</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">organelle</span></p>
          <p>High resolution images of subcellular structures.</p>
          <p><a href="/janelia-cosem/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://science.sciencemag.org/content/367/6475/eaaz5357" class="" target="_none">Correlative three-dimensional super-resolution and block-face electron microscopy of whole vitreously frozen cells.</a> by David P. Hoffman1, Gleb Shtengel, C. Shan Xu, Kirby R. Campbell, Melanie Freeman, Lei Wang, Daniel E. Milkie, H. Amalia Pasolli, Nirmala Iyer, John A. Bogovic, Daniel R. Stabley, Abbas Shirinifard, Song Pang, David Peale, Kathy Schaefer, Wim Pomp, Chi-Lun Chang, Jennifer Lippincott-Schwartz, Tom Kirchhausen1, David J. Solecki, Eric Betzig, Harald F. Hess.</li></a>
          </li>
          <li>
            <a href="https://elifesciences.org/articles/25916" class="" target="_none">Enhanced FIB-SEM systems for large-volume 3D imaging</a> by C. Shan Xu, Kenneth J. Hayworth, Zhiyuan Lu, Patricia Grob, Ahmed M. Hassan, José G. García-Cerdán, Krishna K. Niyogi, Eva Nogales, Richard J. Weinberg, Harald F. Hess.</li></a>
          </li>
        </ul>
        <p><a href="/janelia-cosem/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="rareplanes" class="dataset">
          <h3><a href="/rareplanes/">RarePlanes</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">labeled</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">satellite imagery</span></p>
          <p>RarePlanes is a unique open-source machine learning dataset from CosmiQ Works and AI.Reverie that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of AI.Reverie synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very high resolution dataset built to test the value of synthetic data from an overhead perspective. The real portion ...</p>
          <p><a href="/rareplanes/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/aireveries/RarePlanes" class="" target="_none">RarePlanes Codebase</a> by Thomas Hossler and Jacob Shermeyer</li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2006.02963" class="" target="_none">RarePlanes: Synthetic Data Takes Flight</a> by Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan Lewis, Daeil Kim</li></a>
          </li>
        </ul>
        <p><a href="/rareplanes/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="ford-multi-av-seasonal" class="dataset">
          <h3><a href="/ford-multi-av-seasonal/">Ford Multi-AV Seasonal Dataset</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">lidar</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">robotics</span><span class="label label-info tag link-tag">transportation</span><span class="label label-info tag link-tag">urban</span><span class="label label-info tag link-tag">weather</span></p>
          <p>This research presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. The vehicles The vehicles were manually driven on an average route of 66 km in Michigan that included a mix of driving scenarios like the Detroit Airport, freeways, city-centres, university campus and suburban neighbourhood, etc. Each vehicle used in this data collection  is a Ford Fusion outfitted with an Applanix POS-LV inertial measurement unit (IMU), four HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged on the...</p>
          <p><a href="/ford-multi-av-seasonal/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/Ford/AVData" class="" target="_none">Ford AV Dataset Tutorial</a> by Ford Motor Company</li></a>
          </li>
        </ul>
        <p><a href="/ford-multi-av-seasonal/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="xiph-media" class="dataset">
          <h3><a href="/xiph-media/">Xiph.Org Test Media</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">media</span><span class="label label-info tag link-tag">movies</span><span class="label label-info tag link-tag">multimedia</span></p>
          <p>Uncompressed video used for video compression and video processing research.</p>
          <p><a href="/xiph-media/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://media.xiph.org/aws.html" class="" target="_none">Encoding video with AV1 on EC2</a> by <a href="https://www.xiph.org/" target="_none">Thomas Daede</a></li></a>
          </li>
        </ul>
        <p><a href="/xiph-media/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="aev-a2d2" class="dataset">
          <h3><a href="/aev-a2d2/">A2D2: Audi Autonomous Driving Dataset</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">lidar</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">robotics</span></p>
          <p>An open multi-sensor dataset for autonomous driving research. This dataset comprises semantically segmented images, semantic point clouds, and 3D bounding boxes. In addition, it contains unlabelled 360 degree camera images, lidar, and bus data for three sequences. We hope this dataset will further facilitate active research and development in AI, computer vision, and robotics for autonomous driving.</p>
          <p><a href="/aev-a2d2/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-coco" class="dataset">
          <h3><a href="/fast-ai-coco/">COCO - Common Objects in Context - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>COCO is a large-scale object detection, segmentation, and captioning dataset.
This is part of the fast.ai datasets collection hosted by AWS for convenience
of fast.ai students. If you use this dataset in your research please cite
arXiv:1405.0312 [cs.CV].</p>
          <p><a href="/fast-ai-coco/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-imageclas" class="dataset">
          <h3><a href="/fast-ai-imageclas/">Image classification - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>Some of the most important datasets for image classification research, including
CIFAR 10 and 100, Caltech 101, MNIST, Food-101, Oxford-102-Flowers, Oxford-IIIT-Pets,
and Stanford-Cars.  This is part of the fast.ai datasets collection hosted by
AWS for convenience of fast.ai students. See documentation link for citation and
license details for each dataset.</p>
          <p><a href="/fast-ai-imageclas/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-imagelocal" class="dataset">
          <h3><a href="/fast-ai-imagelocal/">Image localization  - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>Some of the most important datasets for image localization  research, including
Camvid and PASCAL VOC (2007 and 2012). This is part of the fast.ai datasets
collection hosted by AWS for convenience of fast.ai students. See
documentation link for citation and license details for each dataset.</p>
          <p><a href="/fast-ai-imagelocal/">Details &rarr;</a></p>
        </div>
        <div id="kitti" class="dataset">
          <h3><a href="/kitti/">KITTI Vision Benchmark Suite</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">robotics</span></p>
          <p>Dataset and benchmarks for computer vision research in the context of autonomous driving. The dataset has been recorded in and around the city of Karlsruhe, Germany using the mobile platform AnnieWay (VW station wagon) which has been equipped with several RGB and monochrome cameras, a Velodyne HDL 64 laser scanner as well as an accurate RTK corrected GPS/IMU localization unit. The dataset has been created for computer vision and machine learning research on stereo, optical flow, visual odometry, semantic segmentation, semantic instance segmentation, road segmentation, single image depth predic...</p>
          <p><a href="/kitti/">Details &rarr;</a></p>
        </div>
        <div id="multimedia-commons" class="dataset">
          <h3><a href="/multimedia-commons/">Multimedia Commons</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">multimedia</span></p>
          <p>The Multimedia Commons is a collection of audio and visual features computed for the nearly 100 million Creative Commons-licensed Flickr images and videos in the YFCC100M dataset from Yahoo! Labs, along with ground-truth annotations for selected subsets. The International Computer Science Institute (ICSI) and Lawrence Livermore National Laboratory are producing and distributing a core set of derived feature sets and annotations as part of an effort to enable large-scale video search capabilities. They have released this feature corpus into the public domain, under Creative Commons License 0, s...</p>
          <p><a href="/multimedia-commons/">Details &rarr;</a></p>
        </div>
        <div id="mevadata" class="dataset">
          <h3><a href="/mevadata/">Multiview Extended Video with Activities (MEVA)</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">urban</span><span class="label label-info tag link-tag">us</span></p>
          <p>The Multiview Extended Video with Activities (MEVA) dataset consists
video data of human activity, both scripted and unscripted,
collected with roughly 100 actors over several weeks.  The data was
collected with 29 cameras with overlapping and non-overlapping
fields of view. The current release consists of about 328 hours
(516GB, 4259 clips) of video data, as well as 4.6 hours (26GB) of
UAV data. Other data includes GPS tracks of actors, camera models,
and a site map. We have also released annotations for 22 hours of
data. Further updates are planned.</p>
          <p><a href="/mevadata/">Details &rarr;</a></p>
        </div>
        <div id="nsd" class="dataset">
          <h3><a href="/nsd/">Natural Scenes Dataset</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span><span class="label label-info tag link-tag">nifti</span></p>
          <p>Here, we collected and pre-processed a massive, high-quality 7T fMRI dataset that can be used to advance our understanding of how the brain works. A unique feature of this dataset is the massive amount of data available per individual subject. The data were acquired using ultra-high-field fMRI (7T, whole-brain, 1.8-mm resolution, 1.6-s TR). We measured fMRI responses while each of 8 participants viewed 9,000–10,000 distinct, color natural scenes (22,500–30,000 trials) in 30–40 weekly scan sessions over the course of a year. Additional measures were collected including resting-state data, retin...</p>
          <p><a href="/nsd/">Details &rarr;</a></p>
        </div>
        <div id="mmid" class="dataset">
          <h3><a href="/mmid/">The Massively Multilingual Image Dataset (MMID)</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">machine translation</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>MMID is a large-scale, massively multilingual dataset of images paired with the words they represent collected at the <a href="https://upenn.edu">University of Pennsylvania</a>.
The dataset is doubly parallel: for each language, words are stored parallel to images that represent the word, <em>and</em> parallel to the word&#39;s translation into English (and corresponding images.)</p>
          <p><a href="/mmid/">Details &rarr;</a></p>
        </div>
        <div id="agriculture_vision" class="dataset">
          <h3><a href="/agriculture_vision/">AgricultureVision</a></h3>
          <p><span class="label label-info tag link-tag">aerial imagery</span><span class="label label-info tag link-tag">agriculture</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span></p>
          <p>Dataset associated with the paper &quot;Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis&quot;. Agriculture-Vision aims to be a publicly available large-scale aerial agricultural image dataset that is high-resolution, multi-band, and with multiple types of patterns annotated by agronomy experts. In its current stage, we have captured 94,986 512x512images sampled from 3,432 farmlands with nine types of annotations: double plant, drydown, endrow, nutrient deficiency, planter skip, storm damage, water, waterway and weed cluster. All of these patterns have substa...</p>
          <p><a href="/agriculture_vision/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/2001.01306" class="" target="_none">Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis</a> by Mang Tik Chiu, et al.</li></a>
          </li>
        </ul>
        <p><a href="/agriculture_vision/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
      </div>
    </div>


    <hr/>
  </div>  </body>
  <script src="/js/index.js"></script>
</html>
