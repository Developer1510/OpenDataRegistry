<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
  
    
  
    <link rel="alternate" type="application/rss+xml" href="/rss.xml" />
    
    <link rel="shortcut icon" type="image/ico" href="https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico"/>
    <link rel="apple-touch-icon" sizes="57x57" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="72x72" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>
    <link rel="apple-touch-icon" sizes="114x114" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="144x144" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>  
  
    <title>Registry of Open Data on AWS</title>
  
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://assets.opendata.aws/css/bootstrap/3.4.1/bootstrap.min.css">
  
    <!-- Our local CSS -->
    <link rel="stylesheet" href="/css/main.css">
  
  
  </head>
  <body>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://assets.opendata.aws/js/jquery/3.5.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://assets.opendata.aws/js/bootstrap/3.4.1/bootstrap.min.js"></script>
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="roda-header">
          <h2><a href="/" alt="Home">Registry of Open Data on AWS</a></h2>
          <a href="http://aws.amazon.com/what-is-cloud-computing">
            <img src="https://assets.opendata.aws/img/AWS-Logo_White-Color_300x180.png" alt="Powered by AWS Cloud Computing" id="aws_header_logo">
          </a>
        </div>
      </div>
    </nav>
    <div class="container" >

    <div class="about col-md-5">
      <div class="row aboutbox">
            <h3>About</h3>
            <p>This registry exists to help people discover and share datasets that are available via AWS resources. <a href="https://opendata.aws">Learn more about sharing data on AWS</a>.</p>
    
            <p>See <a href="/tag/image-processing/usage-examples">all usage examples for datasets listed in this registry</a> tagged with <strong>image processing</strong>.</p>
    
        <hr>
    
        <h4>Search datasets (currently <span id="count-matching">13</span> matching <span id="count-matching-text">datasets</span>)</h4>
        <form>
          <div class="form-group">
            <input type="text" id="search-box" class="form-control" placeholder="Search datasets" spellcheck="false" autocorrect="off">
          </div>
        </form>
        <p>You are currently viewing a subset of data tagged with <strong>image processing</strong>.</p>
    
        <hr>
    
        <h4>Add to this registry</h4>
        <p>If you want to add a dataset or example of how to use a dataset to this registry, please follow the instructions on the <a href="https://github.com/awslabs/open-data-registry/">Registry of Open Data on AWS GitHub repository</a>.</p>
    
        <p>Unless specifically stated in the applicable dataset documentation, datasets available through the Registry of Open Data on AWS are not provided and maintained by AWS. Datasets are provided and maintained by a variety of third parties under a variety of licenses. Please check dataset licenses and related documentation to determine if a dataset may be used for your application.</p>
      </div>
    </div>
    <div class="col-md-6 col-md-offset-1 datasets">
      <div class="row">
        <div id="allen-cell-imaging-collections" class="dataset">
          <h3><a href="/allen-cell-imaging-collections/">Allen Cell Imaging Collections</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">cell biology</span><span class="label label-info tag link-tag">cell imaging</span><span class="label label-info tag link-tag">Homo sapiens</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">microscopy</span></p>
          <p>This bucket contains multiple datasets (as Quilt packages) created by the
Allen Institute for Cell Science (AICS). The imaging data in this bucket contains
either of the following:1) field of view images from glass plates
2) cell membrane, DNA, and structure segmentations
3) cell membrane, DNA and structure contours
4) machine learning imaging predictions of the previously listed modalities.In addition, many of the datasets include CSVs that contain feature sets
related to that data.</p>
          <p><a href="/allen-cell-imaging-collections/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://drive.google.com/file/d/1nuiYOurmbic7rZeHPen_M3fDbsN7n5Ws/view?usp&#x3D;sharing" class="" target="_none">Integrating single-cell sequencing and nuclear imaging data</a> by <a href="https://noble.gs.washington.edu/" target="_none">Chengxiang Qiu and William Noble</a></li></a>
          </li>
          <li>
            <a href="https://www.allencell.org/visual-guide-to-human-cells.html" class="" target="_none">Visual Guide to Human Cells</a> by <a href="https://www.alleninstitute.org/what-we-do/cell-science/about/team/" target="_none">Allen Institute for Cell Science</a></li></a>
          </li>
          <li>
            <a href="https://github.com/AllenInstitute/volume-viewer" class="" target="_none">AICS Volume Viewer</a> by <a href="https://alleninstitute.org/what-we-do/cell-science/about/team/staff-profiles/daniel-toloudis/" target="_none">Dan Toloudis</a></li></a>
          </li>
          <li>
            <a href="https://github.com/AllenCellModeling/pytorch_fnet/blob/master/examples/download_and_train.py" class="" target="_none">Download and train label-free models</a> by <a href="https://www.alleninstitute.org/what-we-do/cell-science/about/team/staff-profiles/greg-johnson/" target="_none">Greg Johnson</a></li></a>
          </li>
          <li>
            <a href="https://www.nature.com/articles/s41592-018-0111-2" class="" target="_none">Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy</a> by <a href="https://alleninstitute.org/about/team/" target="_none">Chawin Ounkomol, Sharmishtaa Seshamani, Mary M. Maleckar, Forrest Collman &amp; Gregory R. Johnson</a></li></a>
          </li>
        </ul>
        <p><a href="/allen-cell-imaging-collections/#usageexamples">See 11 usage examples &rarr;</a></p>
        </div>
        <div id="janelia-flylight" class="dataset">
          <h3><a href="/janelia-flylight/">Fly Brain Anatomy: FlyLight Gen1 and Split-GAL4 Imagery</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">fluorescence imaging</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">microscopy</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span></p>
          <p>This data set, made available by Janelia&#39;s FlyLight project, consists of fluorescence images 
of Drosophila melanogaster driver lines, aligned to standard templates, and stored in formats 
suitable for rapid searching and visualization. Additional data will be added as it is published. 
A large release of Gen1 MCFO samples is coming at the beginning of May 2020. </p>
          <p><a href="/janelia-flylight/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://doi.org/10.7554/eLife.04577" class="" target="_none">The neuronal architecture of the mushroom body provides a logic for associative learning</a> by Aso, Y., Hattori, D., Yu, Y., Johnston, R. M., Iyer, N. A., Ngo, T., Dionne, H., Abbott, L., Axel, R., Tanimoto, H., &amp; Rubin, G. M.</li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1016/j.celrep.2012.09.011" class="" target="_none">A GAL4-Driver Line Resource for Drosophila Neurobiology</a> by Jenett, A., Rubin, G. M., Ngo, T., Shepherd, D., Murphy, C., Dionne, H., Pfeiffer, B. D., Cavallaro, A., Hall, D., Jeter, J., Iyer, N., Fetter, D., Hausenfluck, J. H., Peng, H., Trautman, E. T., Svirskas, R. R., Myers, E. W., Iwinski, Z. R., Aso, Y., DePasquale, G. M., Enos, A., Hulamm, P., Lam, S. C. B., Li, H., Laverty, T. R., Long, F., Qu, L., Murphy, S. D., Rokicki, K., Safford, T., Shaw, K., Simpson, J. H., Sowell, A., Tae, S., Yu, Y., &amp; Zugates, C. T.</li></a>
          </li>
          <li>
            <a href="https://www.janelia.org/project-team/flylight" class="" target="_none">FlyLight Project Website</a> by Geoffrey Meissner</li></a>
          </li>
          <li>
            <a href="https://github.com/JaneliaSciComp/open-data-flylight/blob/master/tutorials/Using_imagery_on_AWS_S3.ipynb" class="" target="_none">Using Imagery on AWS S3</a> by Rob Svirskas</li></a>
          </li>
          <li>
            <a href="http://splitgal4.janelia.org/cgi-bin/splitgal4.cgi" class="" target="_none">Fly Light Split-GAL4 Driver Collection</a> by Rob Svirskas</li></a>
          </li>
        </ul>
        <p><a href="/janelia-flylight/#usageexamples">See 10 usage examples &rarr;</a></p>
        </div>
        <div id="open-neurodata" class="dataset">
          <h3><a href="/open-neurodata/">Open NeuroData</a></h3>
          <p><span class="label label-info tag link-tag">array tomography</span><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">electron microscopy</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">lightsheet microscopy</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span></p>
          <p>This bucket contains multiple neuroimaging datasets (as Neuroglancer Precomputed Volumes) across multiple modalities and scales, ranging from nanoscale (electron microscopy), to microscale (cleared lightsheet microscopy and array tomography), and mesoscale (structural and functional magnetic resonance imaging). Additionally, many of the datasets include segmentations and meshes.</p>
          <p><a href="/open-neurodata/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://neurodata.io/help/visualization/" class="" target="_none">Visualization using Neuroglancer</a> by <a href="https://github.com/falkben" target="_none">Benjamin Falk</a></li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1016/j.neuron.2016.10.033" class="" target="_none">To the Cloud! A Grassroots Proposal to Accelerate Brain Science Discovery</a> by J. T. Vogelstein, B. Mensh, M. Häusser, N. Spruston, A. C. Evans, K. Kording, K. Amunts, C. Ebell, J. Muller, M. Telefont, S. Hill, S. P. Koushika, C. Calì, P. A. Valdés-Sosa, P. B. Littlewood, C. Koch, S. Saalfeld, A. Kepecs, H. Peng, Y. O. Halchenko, G. Kiar, M. M. Poo, J. B. Poline, M. P. Milham, A. P. Schaffer, R. Gidron, H. Okano, V. D. Calhoun, M. Chun, D. M. Kleissas, R. J. Vogelstein, E. Perlman, R. Burns, R. Huganir, and M. I. Miller</li></a>
          </li>
          <li>
            <a href="https://github.com/seung-lab/cloud-volume" class="" target="_none">CloudVolume</a> by <a href="https://github.com/william-silversmith" target="_none">William Silversmith</a></li></a>
          </li>
          <li>
            <a href="https://www.nature.com/articles/s41592-018-0181-1" class="" target="_none">A Community-Developed Open-Source Computational Ecosystem for Big Neuro Data</a> by J. T. Vogelstein, E. Perlman, B. Falk, A. Baden, W. Gray Roncal, V. Chandrashekhar, F. Collman, S. Seshamani, J. L. Patsolic, K. Lillaney, M. Kazhdan, R. Hider, D. Pryor, J. Matelsky, T. Gion, P. Manavalan, B. Wester, M. Chevillet, E. T. Trautman, K. Khairy, E. Bridgeford, D. M. Kleissas, D. J. Tward, A. K. Crow, B. Hsueh, M. A. Wright, M. I. Miller, S. J. Smith, R. J. Vogelstein, K. Deisseroth, and R. Burns</li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/1306.3543" class="" target="_none">The Open Connectome Project Data Cluster: Scalable Analysis and Vision for High-Throughput Neuroscience</a> by R. Burns, W. G. Roncal, D. Kleissas, K. Lillaney, P. Manavalan, E. Perlman, D. R. Berger, D. D. Bock, K. Chung, L. Grosenick, N. Kasthuri, N. C. Weiler, K. Deisseroth, M. Kazhdan, J. Lichtman, R. C. Reid, S. J. Smith, A. S. Szalay, J. T. Vogelstein, and R. J. Vogelstein.</li></a>
          </li>
        </ul>
        <p><a href="/open-neurodata/#usageexamples">See 9 usage examples &rarr;</a></p>
        </div>
        <div id="ladi" class="dataset">
          <h3><a href="/ladi/">Low Altitude Disaster Imagery (LADI) Dataset</a></h3>
          <p><span class="label label-info tag link-tag">aerial imagery</span><span class="label label-info tag link-tag">coastal</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">earthquakes</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">infrastructure</span><span class="label label-info tag link-tag">land</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">natural resource</span><span class="label label-info tag link-tag">seismology</span><span class="label label-info tag link-tag">transportation</span><span class="label label-info tag link-tag">urban</span><span class="label label-info tag link-tag">water</span></p>
          <p>The Low Altitude Disaster Imagery (LADI) Dataset consists of human and machine annotated airborne images collected by the Civil Air Patrol in support of various disaster responses from 2015-2019. The initial release of LADI focuses on the Atlantic hurricane seasons and coastal states along the Atlantic Ocean and Gulf of Mexico. Annotations are included for major hurricanes of Harvey, Maria, and Florence. Two key distinctions are the low altitude, oblique perspective of the imagery and disaster-related features, which are rarely featured in computer vision benchmarks and datasets.</p>
          <p><a href="/ladi/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://doi.org/10.1109/HPEC.2019.8916437" class="" target="_none">Large Scale Organization and Inference of an Imagery Dataset for Public Safety</a> by Jeffrey Liu, David Strohschein, Siddharth Samsi, Andrew Weinert</li></a>
          </li>
          <li>
            <a href="https://www-nlpir.nist.gov/projects/tv2020/dsdi.html" class="" target="_none">NIST TRECVID 2020 - Disaster Scene Description and Indexing (DSDI)</a> by TREC Video Retrieval Evaluation (TRECVID)</li></a>
          </li>
          <li>
            <a href="https://github.com/LADI-Dataset/ladi-tutorial" class="" target="_none">LADI Tutorials</a> by <a href="https://github.com/LADI-Dataset" target="_none">Andrew Weinert, Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren, Ryan Earley, Nadia Dimitrova</a></li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2005.05495" class="" target="_none">Train and Deploy an Image Classifier for Disaster Response</a> by Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren</li></a>
          </li>
          <li>
            <a href="https://github.com/bwsi-hadr" class="" target="_none">Remote Sensing for Disaster Response Course</a> by <a href="https://beaverworks.ll.mit.edu/CMS/bw/bwsi" target="_none">Beaver Works Summer Institute</a></li></a>
          </li>
        </ul>
        <p><a href="/ladi/#usageexamples">See 6 usage examples &rarr;</a></p>
        </div>
        <div id="nyu-fastmri" class="dataset">
          <h3><a href="/nyu-fastmri/">NYU Langone &amp; FAIR FastMRI Dataset</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">health</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">neuroimaging</span></p>
          <p>This dataset contains deidentified raw k-space data and DICOM image files of over 1,500 knees and 6,970 brains.</p>
          <p><a href="/nyu-fastmri/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/1904.01112" class="" target="_none">Deep Learning Methods for Parallel Magnetic Resonance Image Reconstruction</a> by <a href="https://arxiv.org/search/eess?searchtype&#x3D;author&amp;query&#x3D;Knoll%2C+F" target="_none">Knoll et al (2019)</a></li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/1811.08839" class="" target="_none">fastMRI:An Open Dataset and Benchmarks for Accelerated MRI</a> by <a href="https://arxiv.org/search/cs?searchtype&#x3D;author&amp;query&#x3D;Zbontar%2C+J" target="_none">Zbontar et al (2019)</a></li></a>
          </li>
          <li>
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/29774597" class="" target="_none">Assessment of the generalization of learned image reconstruction and the potential for transfer learning.</a> by <a href="https://arxiv.org/search/eess?searchtype&#x3D;author&amp;query&#x3D;Knoll%2C+F" target="_none">Knoll et al (2019)</a></li></a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/fastMRI/blob/master/fastMRI_tutorial.ipynb" class="" target="_none">FastMRI Tutorial (Jupyter Notebook)</a> by <a href="https://scholar.google.com/citations?user&#x3D;LjjLn2YAAAAJ&amp;hl&#x3D;en" target="_none">Tullie Murrell</a></li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2001.02518" class="" target="_none">Advancing machine learning for MR image reconstruction with an open competition:Overview of the 2019 fastMRI challenge</a> by <a href="https://arxiv.org/search/eess?searchtype&#x3D;author&amp;query&#x3D;Knoll%2C+F" target="_none">Knoll et al (2020)</a></li></a>
          </li>
        </ul>
        <p><a href="/nyu-fastmri/#usageexamples">See 6 usage examples &rarr;</a></p>
        </div>
        <div id="allen-mouse-brain-atlas" class="dataset">
          <h3><a href="/allen-mouse-brain-atlas/">Allen Mouse Brain Atlas</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">gene expression</span><span class="label label-info tag link-tag">genetic</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">Mus musculus</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">transcriptomics</span></p>
          <p>The Allen Mouse Brain Atlas is a genome-scale collection of cellular resolution gene expression profiles using in situ hybridization (ISH). Highly methodical data production methods and comprehensive anatomical coverage via dense, uniformly spaced sampling facilitate data consistency and comparability across &gt;20,000 genes. The use of an inbred mouse strain with minimal animal-to-animal variance allows one to treat the brain essentially as a complex but highly reproducible three-dimensional tissue array. The entire Allen Mouse Brain Atlas dataset and associated tools are available through an...</p>
          <p><a href="/allen-mouse-brain-atlas/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/AllenInstitute/open_dataset_tools/blob/master/Visualizing_Images_from_Allen_Mouse_Brain_Atlas.ipynb" class="" target="_none">Visualizing Images from the Allen Mouse Brain Atlas</a> by <a href="www.alleninstitute.org" target="_none">Allen Institute for Brain Science</a></li></a>
          </li>
          <li>
            <a href="https://mouse.brain-map.org" class="" target="_none">Allen Mouse Brain Atlas</a> by <a href="www.alleninstitute.org" target="_none">Allen Institute for Brain Science</a></li></a>
          </li>
          <li>
            <a href="http://www.nature.com/articles/nature05453" class="" target="_none">Genome-wide atlas of gene expression in the adult mouse brain</a> by <a href="www.alleninstitute.org" target="_none">Ed Lein, et al.</a></li></a>
          </li>
        </ul>
        <p><a href="/allen-mouse-brain-atlas/#usageexamples">See 3 usage examples &rarr;</a></p>
        </div>
        <div id="nrel-pds-porotomo" class="dataset">
          <h3><a href="/nrel-pds-porotomo/">PoroTomo</a></h3>
          <p><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">geothermal</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">seismology</span></p>
          <p>Released to the public as part of the Department of Energy&#39;s Open Energy Data
Initiative, these data represent vertical and horizontal distributed acoustic
sensing (DAS) data collected as part of the Poroelastic Tomography (PoroTomo)
project funded in part by the Office of Energy Efficiency and Renewable
Energy (EERE), U.S. Department of Energy.</p>
          <p><a href="/nrel-pds-porotomo/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://pangea.stanford.edu/ERE/pdf/IGAstandard/SGW/2018/Miller.pdf" class="" target="_none">DAS and DTS at Brady Hot Springs: Observations about Coupling and Coupled Interpretations</a> by Douglas E. Miller, Thomas Coleman, Xiangfang Zeng, Jeremy R. Patterson , Elena C. Reinnisch, Michael A. Cardiff, Herbert F. Wang, Dante Fratta, Whitney Trainor-Guitton, Clifford H. Thurber, Michelle ROBERTSON, Kurt FEIGL, and The PoroTomo Team</li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1093/gji/ggy182" class="" target="_none">Ground motion response to an ML 4.3 earthquake using co-located distributed acoustic sensing and seismometer arrays</a> by Herbert F Wang, Xiangfang Zeng, Douglas E Miller, Dante Fratta, Kurt L Feigl, Clifford H Thurber, Robert J Mellors</li></a>
          </li>
          <li>
            <a href="https://github.com/openEDI/documentation/blob/master/PoroTomo_Distributed_Acoustic_Sensing_(DAS)_Data.ipynb" class="" target="_none">PoroTomo DAS Data Processing Tutorial</a> by <a href="https://orcid.org/0000-0001-7776-2028" target="_none">Nicole Taverna and Ross Ring-Jarvi</a></li></a>
          </li>
        </ul>
        <p><a href="/nrel-pds-porotomo/#usageexamples">See 3 usage examples &rarr;</a></p>
        </div>
        <div id="digitalcorpora" class="dataset">
          <h3><a href="/digitalcorpora/">DigitalCorpora</a></h3>
          <p><span class="label label-info tag link-tag">computer forensics</span><span class="label label-info tag link-tag">computer security</span><span class="label label-info tag link-tag">CSI</span><span class="label label-info tag link-tag">cyber security</span><span class="label label-info tag link-tag">digital forensics</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">information retrieval</span><span class="label label-info tag link-tag">internet</span><span class="label label-info tag link-tag">intrusion detection</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">machine translation</span><span class="label label-info tag link-tag">text analysis</span></p>
          <p>Disk images, memory dumps, network packet captures, and files for use in digital forensics research and education. All of this information is accessible through the digitalcorpora.org website, and made available at s3://digitalcorpora/. Some of these datasets implement scenarios that were performed by students, faculty, and others acting <em>in persona</em>. As such, the information is synthetic and may be used without prior authorization or IRB approval. Details of these datasets can be found at <a href="http://www.simson.net/clips/academic/2009...</p>
          <p><a href="/digitalcorpora/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="http://simson.net/clips/academic/2011.ADFSL.Corpora.pdf" class="" target="_none">Creating Realistic Corpora for Forensic and Security Education</a> by <a href="https://simson.net/" target="_none">Woods, K., Christopher Lee, Simson Garfinkel, David Dittrich, Adam Russel, Kris Kearton</a></li></a>
          </li>
          <li>
            <a href="http://www.simson.net/clips/academic/2009.DFRWS.Corpora.pdf" class="" target="_none">Bringing Science to Digital Forensics with Standardized Forensic Corpora</a> by <a href="https://simson.net/" target="_none">Garfinkel, Farrell, Roussev and Dinolt</a></li></a>
          </li>
        </ul>
        <p><a href="/digitalcorpora/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="allen-brain-observatory" class="dataset">
          <h3><a href="/allen-brain-observatory/">Allen Brain Observatory - Visual Coding AWS Public Data Set</a></h3>
          <p><span class="label label-info tag link-tag">electrophysiology</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">Mus musculus</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">signal processing</span></p>
          <p>The Allen Brain Observatory – Visual Coding is a large-scale, standardized survey of physiological activity across the mouse visual cortex, hippocampus, and thalamus. It includes datasets collected with both two-photon imaging and Neuropixels probes, two complementary techniques for measuring the activity of neurons in vivo. The two-photon imaging dataset features visually evoked calcium responses from GCaMP6-expressing neurons in a range of cortical layers, visual areas, and Cre lines. The Neuropixels dataset features spiking activity from distributed cortical and subcortical brain regions, c...</p>
          <p><a href="/allen-brain-observatory/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/AllenInstitute/AllenSDK/wiki/Use-the-Allen-Brain-Observatory-%E2%80%93-Visual-Coding-on-AWS" class="" target="_none">Use the Allen Brain Observatory – Visual Coding on AWS</a> by <a href="https://twitter.com/AllenInstitute" target="_none">Nika Keller, David Feng</a></li></a>
          </li>
        </ul>
        <p><a href="/allen-brain-observatory/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="ocmr_data" class="dataset">
          <h3><a href="/ocmr_data/">Ohio State Cardiac MRI Raw Data (OCMR)</a></h3>
          <p><span class="label label-info tag link-tag">Homo sapiens</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">signal processing</span></p>
          <p>OCMR is an open-access repository that provides multi-coil k-space data for cardiac cine.  The fully sampled MRI datasets are intended for quantitative comparison and evaluation of image reconstruction methods. The free-breathing, prospectively undersampled datasets are intended to evaluate their performance and generalizability qualitatively.</p>
          <p><a href="/ocmr_data/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/MRIOSU/OCMR/blob/master/Python/example_ocmr.ipynb" class="" target="_none">OCMR Tutorial</a> by <a href="https://cmr.engineering.osu.edu/people/trainees" target="_none">Chong Chen</a></li></a>
          </li>
        </ul>
        <p><a href="/ocmr_data/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="xiph-media" class="dataset">
          <h3><a href="/xiph-media/">Xiph.Org Test Media</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">media</span><span class="label label-info tag link-tag">movies</span><span class="label label-info tag link-tag">multimedia</span></p>
          <p>Uncompressed video used for video compression and video processing research.</p>
          <p><a href="/xiph-media/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://media.xiph.org/aws.html" class="" target="_none">Encoding video with AV1 on EC2</a> by <a href="https://www.xiph.org/" target="_none">Thomas Daede</a></li></a>
          </li>
        </ul>
        <p><a href="/xiph-media/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="dataforgood-fb-hrsl" class="dataset">
          <h3><a href="/dataforgood-fb-hrsl/">High Resolution Population Density Maps + Demographic Estimates by CIESIN and Facebook</a></h3>
          <p><span class="label label-info tag link-tag">aerial imagery</span><span class="label label-info tag link-tag">demographics</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">population</span><span class="label label-info tag link-tag">satellite imagery</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>Population data for a selection of countries, allocated to 1 arcsecond blocks and provided in a combination of CSV
and Cloud-optimized GeoTIFF files. This refines <a href="https://sedac.ciesin.columbia.edu/data/collection/gpw-v4">CIESIN’s Gridded Population of the World</a>
using machine learning models on high-resolution worldwide Digital Globe
satellite imagery. CIESIN population counts aggregated from worldwide census
data are allocated to blocks where imagery appears to contain buildings.</p>
          <p><a href="/dataforgood-fb-hrsl/">Details &rarr;</a></p>
        </div>
        <div id="nsd" class="dataset">
          <h3><a href="/nsd/">Natural Scenes Dataset</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span><span class="label label-info tag link-tag">nifti</span></p>
          <p>Here, we collected and pre-processed a massive, high-quality 7T fMRI dataset that can be used to advance our understanding of how the brain works. A unique feature of this dataset is the massive amount of data available per individual subject. The data were acquired using ultra-high-field fMRI (7T, whole-brain, 1.8-mm resolution, 1.6-s TR). We measured fMRI responses while each of 8 participants viewed 9,000–10,000 distinct, color natural scenes (22,500–30,000 trials) in 30–40 weekly scan sessions over the course of a year. Additional measures were collected including resting-state data, retin...</p>
          <p><a href="/nsd/">Details &rarr;</a></p>
        </div>
      </div>
    </div>


    <hr/>
  </div>  </body>
  <script src="/js/index.js"></script>
</html>
