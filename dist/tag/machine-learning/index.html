<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
  
    
  
    <link rel="alternate" type="application/rss+xml" href="/rss.xml" />
    
    <link rel="shortcut icon" type="image/ico" href="https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico"/>
    <link rel="apple-touch-icon" sizes="57x57" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="72x72" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>
    <link rel="apple-touch-icon" sizes="114x114" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="144x144" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>  
  
    <title>Registry of Open Data on AWS</title>
  
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://assets.opendata.aws/css/bootstrap/3.4.1/bootstrap.min.css">
  
    <!-- Our local CSS -->
    <link rel="stylesheet" href="/css/main.css">
  
  
  </head>
  <body>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://assets.opendata.aws/js/jquery/3.5.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://assets.opendata.aws/js/bootstrap/3.4.1/bootstrap.min.js"></script>
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="roda-header">
          <h2><a href="/" alt="Home">Registry of Open Data on AWS</a></h2>
          <a href="http://aws.amazon.com/what-is-cloud-computing">
            <img src="https://assets.opendata.aws/img/AWS-Logo_White-Color_300x180.png" alt="Powered by AWS Cloud Computing" id="aws_header_logo">
          </a>
        </div>
      </div>
    </nav>
    <div class="container" >

    <div class="about col-md-5">
      <div class="row aboutbox">
            <h3>About</h3>
            <p>This registry exists to help people discover and share datasets that are available via AWS resources. <a href="https://opendata.aws">Learn more about sharing data on AWS</a>.</p>
    
            <p>See <a href="/tag/machine-learning/usage-examples">all usage examples for datasets listed in this registry</a> tagged with <strong>machine learning</strong>.</p>
    
        <hr>
    
        <h4>Search datasets (currently <span id="count-matching">13</span> matching <span id="count-matching-text">datasets</span>)</h4>
        <form>
          <div class="form-group">
            <input type="text" id="search-box" class="form-control" placeholder="Search datasets" spellcheck="false" autocorrect="off">
          </div>
        </form>
        <p>You are currently viewing a subset of data tagged with <strong>machine learning</strong>.</p>
    
        <hr>
    
        <h4>Add to this registry</h4>
        <p>If you want to add a dataset or example of how to use a dataset to this registry, please follow the instructions on the <a href="https://github.com/awslabs/open-data-registry/">Registry of Open Data on AWS GitHub repository</a>.</p>
    
        <p>Unless specifically stated in the applicable dataset documentation, datasets available through the Registry of Open Data on AWS are not provided and maintained by AWS. Datasets are provided and maintained by a variety of third parties under a variety of licenses. Please check dataset licenses and related documentation to determine if a dataset may be used for your application.</p>
      </div>
    </div>
    <div class="col-md-6 col-md-offset-1 datasets">
      <div class="row">
        <div id="commoncrawl" class="dataset">
          <h3><a href="/commoncrawl/">Common Crawl</a></h3>
          <p><span class="label label-info tag link-tag">encyclopedic</span><span class="label label-info tag link-tag">internet</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>A corpus of web crawl data composed of over 50 billion web pages.</p>
          <p><a href="/commoncrawl/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://psuter.net/2019/07/07/z-index" class="" target="_none">Index fun</a> by Philippe Suter</li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2010.12002" class="" target="_none">On the impact of publicly available news and information transfer to financial markets</a> by Metod Jazbec, Barna Pásztor, Felix Faltings, Nino Antulov-Fantulin, Petter N. Kolm</li></a>
          </li>
          <li>
            <a href="https://www.aclweb.org/anthology/2020.emnlp-main.480" class="" target="_none">CCAligned: A Massive collection of cross-lingual web-document pairs</a> by <a href="http://www.statmt.org/cc-aligned/" target="_none">Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, Philipp Koehn</a></li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1145/3178876.3186090" class="" target="_none">Large-scale analysis of style injection by relative path overwrite</a> by Sajjad Arshad, Seyed Ali Mirheidari, Tobias Lauinger, Bruno Crispo, Engin Kirda, William Robertson</li></a>
          </li>
          <li>
            <a href="https://skeptric.com/common-crawl-index-athena/" class="" target="_none">Common Crawl Index Athena</a> by Edward Ross</li></a>
          </li>
        </ul>
        <p><a href="/commoncrawl/#usageexamples">See 23 usage examples &rarr;</a></p>
        </div>
        <div id="allen-cell-imaging-collections" class="dataset">
          <h3><a href="/allen-cell-imaging-collections/">Allen Cell Imaging Collections</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">cell biology</span><span class="label label-info tag link-tag">cell imaging</span><span class="label label-info tag link-tag">Homo sapiens</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">microscopy</span></p>
          <p>This bucket contains multiple datasets (as Quilt packages) created by the
Allen Institute for Cell Science (AICS). The imaging data in this bucket contains
either of the following:1) field of view images from glass plates
2) cell membrane, DNA, and structure segmentations
3) cell membrane, DNA and structure contours
4) machine learning imaging predictions of the previously listed modalities.In addition, many of the datasets include CSVs that contain feature sets
related to that data.</p>
          <p><a href="/allen-cell-imaging-collections/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://www.biorxiv.org/content/10.1101/2020.12.08.415562v1" class="" target="_none">Robust integrated intracellular organization of the human iPS cell: where, how much, and how variable?</a> by <a href="https://www.alleninstitute.org/what-we-do/cell-science/about/team/" target="_none">Matheus P. Viana, Jianxu Chen*, Theo A. Knijnenburg*, Ritvik Vasan*, Calysta Yan*... Allen Institute for Cell Science... Graham T. Johnson, Ruwanthi N. Gunawardane, Nathalie Gaudreault, Julie A. Theriot, Susanne M. Rafelski</a></li></a>
          </li>
          <li>
            <a href="https://github.com/AllenInstitute/volume-viewer" class="" target="_none">AICS Volume Viewer</a> by <a href="https://alleninstitute.org/what-we-do/cell-science/about/team/staff-profiles/daniel-toloudis/" target="_none">Dan Toloudis</a></li></a>
          </li>
          <li>
            <a href="https://cfe.allencell.org/" class="" target="_none">Allen Cell Feature Explorer</a> by <a href="https://www.alleninstitute.org/what-we-do/cell-science/about/team/" target="_none">Allen Institute for Cell Science</a></li></a>
          </li>
          <li>
            <a href="https://imsc.allencell.org" class="" target="_none">Integrated Mitotic Stem Cell</a> by <a href="https://www.alleninstitute.org/what-we-do/cell-science/about/team/" target="_none">Allen Institute for Cell Science</a></li></a>
          </li>
          <li>
            <a href="https://github.com/AllenCellModeling/aicsimageio" class="" target="_none">AICSImageIO</a> by <a href="https://www.alleninstitute.org/what-we-do/cell-science/about/team/" target="_none">Matthew Bowden, Jackson Brown, Jamie Sherman, Dan Toloudis</a></li></a>
          </li>
        </ul>
        <p><a href="/allen-cell-imaging-collections/#usageexamples">See 11 usage examples &rarr;</a></p>
        </div>
        <div id="spacenet" class="dataset">
          <h3><a href="/spacenet/">SpaceNet</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">satellite imagery</span></p>
          <p>SpaceNet, launched in August 2016 as an open innovation project offering a repository of freely available
imagery with co-registered map features. Before SpaceNet, computer vision researchers had minimal options
to obtain free, precision-labeled, and high-resolution satellite imagery. Today, SpaceNet hosts datasets
developed by its own team, along with data sets from projects like IARPA’s Functional Map of the World (fMoW).</p>
          <p><a href="/spacenet/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://aws.amazon.com/blogs/machine-learning/extracting-buildings-and-roads-from-aws-open-data-using-amazon-sagemaker/" class="aws-link" target="_none">Extracting buildings and roads from AWS Open Data using Amazon SageMaker</a> by Yunzhi Shi, Tianyu Zhang, and Xin Chen</li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/the-spacenet-7-multi-temporal-urban-development-challenge-dataset-release-9e6e5f65c8d5" class="" target="_none">The SpaceNet 7 Multi-Temporal Urban Development Challenge: Dataset Release</a> by <a href="https://medium.com/@avanetten" target="_none">Adam Van Etten</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/spacenet-6-dataset-release-66076c8fb79b" class="" target="_none">SpaceNet 6: Dataset Release</a> by <a href="https://medium.com/@jshermeyer" target="_none">Jake Shermeyer</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/announcing-solaris-an-open-source-python-library-for-analyzing-overhead-imagery-with-machine-48c1489c29f7" class="" target="_none">Solaris: an open source Python library for analyzing overhead imagery with machine learning</a> by Nick Weir</li></a>
          </li>
          <li>
            <a href="https://medium.com/the-downlinq/spacenet-5-dataset-release-80bd82d7c528" class="" target="_none">SpaceNet 5 Dataset Release</a> by <a href="https://medium.com/@avanetten" target="_none">Adam Van Etten and Ryan Lewis</a></li></a>
          </li>
        </ul>
        <p><a href="/spacenet/#usageexamples">See 11 usage examples &rarr;</a></p>
        </div>
        <div id="ladi" class="dataset">
          <h3><a href="/ladi/">Low Altitude Disaster Imagery (LADI) Dataset</a></h3>
          <p><span class="label label-info tag link-tag">aerial imagery</span><span class="label label-info tag link-tag">coastal</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">earthquakes</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">infrastructure</span><span class="label label-info tag link-tag">land</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">natural resource</span><span class="label label-info tag link-tag">seismology</span><span class="label label-info tag link-tag">transportation</span><span class="label label-info tag link-tag">urban</span><span class="label label-info tag link-tag">water</span></p>
          <p>The Low Altitude Disaster Imagery (LADI) Dataset consists of human and machine annotated airborne images collected by the Civil Air Patrol in support of various disaster responses from 2015-2019. The initial release of LADI focuses on the Atlantic hurricane seasons and coastal states along the Atlantic Ocean and Gulf of Mexico. Annotations are included for major hurricanes of Harvey, Maria, and Florence. Two key distinctions are the low altitude, oblique perspective of the imagery and disaster-related features, which are rarely featured in computer vision benchmarks and datasets.</p>
          <p><a href="/ladi/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://doi.org/10.1109/LNET.2019.2955833" class="" target="_none">Video Testing at the FirstNet Innovation and Test Lab Using a Public Safety Dataset</a> by Chris Budny, Jeffrey Liu, Andrew Weinert</li></a>
          </li>
          <li>
            <a href="https://github.com/LADI-Dataset/ladi-tutorial" class="" target="_none">LADI Tutorials</a> by <a href="https://github.com/LADI-Dataset" target="_none">Andrew Weinert, Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren, Ryan Earley, Nadia Dimitrova</a></li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1109/HPEC.2019.8916437" class="" target="_none">Large Scale Organization and Inference of an Imagery Dataset for Public Safety</a> by Jeffrey Liu, David Strohschein, Siddharth Samsi, Andrew Weinert</li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2005.05495" class="" target="_none">Train and Deploy an Image Classifier for Disaster Response</a> by Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren</li></a>
          </li>
          <li>
            <a href="https://www-nlpir.nist.gov/projects/tv2020/dsdi.html" class="" target="_none">NIST TRECVID 2020 - Disaster Scene Description and Indexing (DSDI)</a> by TREC Video Retrieval Evaluation (TRECVID)</li></a>
          </li>
        </ul>
        <p><a href="/ladi/#usageexamples">See 6 usage examples &rarr;</a></p>
        </div>
        <div id="radiant-mlhub" class="dataset">
          <h3><a href="/radiant-mlhub/">Radiant MLHub</a></h3>
          <p><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">environmental</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">labeled</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">satellite imagery</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>Radiant MLHub is an open library for geospatial training data that hosts datasets generated by <a href="https://www.radiant.earth/">Radiant Earth Foundation</a>&#39;s team as well as other training data catalogs contributed by Radiant Earth’s partners. Radiant MLHub is open to anyone to access, store, register and/or share their training datasets for high-quality Earth observations. All of the training datasets are stored using a <a href="https://stacspec.org/">SpatioTemporal Asset Catalog (STAC)</a> compliant catalog and exposed through a common API. Training datasets include pairs of imagery and labels for different types of machine learning problems including image ...</p>
          <p><a href="/radiant-mlhub/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://aws.amazon.com/blogs/publicsector/geo-diverse-open-training-data-as-a-global-public-good/" class="aws-link" target="_none">Geo-Diverse Open Training Data as a Global Public Good</a> by <a href="https://www.linkedin.com/in/hamedalemohammad/" target="_none">Hamed Alemohammad</a></li></a>
          </li>
          <li>
            <a href="https://github.com/radiantearth/mlhub-tutorials/blob/master/RadiantMLHub-intro.pdf" class="" target="_none">How to access Radiant MLHub Data</a> by <a href="https://www.radiant.earth/" target="_none">Radiant Earth</a></li></a>
          </li>
          <li>
            <a href="https://zindi.africa/competitions/iclr-workshop-challenge-2-radiant-earth-computer-vision-for-crop-recognition" class="" target="_none">Challenge on Computer Vision for Crop Detection from Satellite Imagery</a> by <a href="https://www.radiant.earth/" target="_none">Radiant Earth</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/radiant-earth-insights/creating-a-machine-learning-commons-for-global-development-256ef3dd46aa" class="" target="_none">Creating a Machine Learning Commons for Global Development</a> by <a href="https://www.linkedin.com/in/hamedalemohammad/" target="_none">Hamed Alemohammad</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/radiant-earth-insights/a-guide-for-collecting-and-sharing-ground-reference-data-for-machine-learning-applications-90664930925e" class="" target="_none">A Guide for Collecting and Sharing Ground Reference Data for Machine Learning Applications</a> by <a href="https://www.linkedin.com/in/yonahbg/" target="_none">Yonah Bromberg Gaber</a></li></a>
          </li>
        </ul>
        <p><a href="/radiant-mlhub/#usageexamples">See 6 usage examples &rarr;</a></p>
        </div>
        <div id="ncar-cesm-lens" class="dataset">
          <h3><a href="/ncar-cesm-lens/">Community Earth System Model Large Ensemble (CESM LENS)</a></h3>
          <p><span class="label label-info tag link-tag">atmosphere</span><span class="label label-info tag link-tag">climate</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">ice</span><span class="label label-info tag link-tag">land</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">model</span><span class="label label-info tag link-tag">oceans</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>The Community Earth System Model (CESM) Large Ensemble Numerical Simulation (LENS) dataset includes a 40-member ensemble of climate simulations for the period 1920-2100 using historical data (1920-2005) or assuming the RCP8.5 greenhouse gas concentration scenario (2006-2100), as well as longer control runs based on pre-industrial conditions. The data comprise both surface (2D) and volumetric (3D) variables in the atmosphere, ocean, land, and ice domains. The total data volume of the original dataset is ~500TB, which has traditionally been stored as ~150,000 individual CF/NetCDF files on disk o...</p>
          <p><a href="/ncar-cesm-lens/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/NCAR/cesm-lens-aws" class="" target="_none">Jupyter Notebook and other documentation and tools for CESM LENS on AWS</a> by NCAR Science at Scale team</li></a>
          </li>
          <li>
            <a href="http://ncar-aws-www.s3-website-us-west-2.amazonaws.com/kay-et-al-2015.v2.html" class="" target="_none">Rendered (static) version of Jupyter Notebook</a> by Anderson Banihirwe, NCAR</li></a>
          </li>
          <li>
            <a href="https://doi.org/10.5065/d6j101d1" class="" target="_none">The Community Earth System Model (CESM) Large Ensemble Project: A Community Resource for Studying Climate Change in the Presence of Internal Climate Variability</a> by Kay et al. (2015), Bull. AMS, 96, 1333-1349</li></a>
          </li>
          <li>
            <a href="https://medium.com/pangeo/cesm-lens-on-aws-4e2a996397a1" class="" target="_none">Analyzing large climate model ensembles in the cloud</a> by Joe Hamman, NCAR</li></a>
          </li>
        </ul>
        <p><a href="/ncar-cesm-lens/#usageexamples">See 4 usage examples &rarr;</a></p>
        </div>
        <div id="encode-project" class="dataset">
          <h3><a href="/encode-project/">Encyclopedia of DNA Elements (ENCODE)</a></h3>
          <p><span class="label label-info tag link-tag">bioinformatics</span><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">genetic</span><span class="label label-info tag link-tag">genomic</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>The Encyclopedia of DNA Elements (ENCODE) Consortium is an international collaboration of
research groups funded by the National Human Genome Research Institute (NHGRI). The goal
of ENCODE is to build a comprehensive parts list of functional elements in the human genome,
including elements that act at the protein and RNA levels, and regulatory elements that
control cells and circumstances in which a gene is active. ENCODE investigators employ a
variety of assays and methods to identify functional elements. The discovery and annotation
of gene elements is accomplished primarily by sequencing a ...</p>
          <p><a href="/encode-project/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/ENCODE-DCC/encode-data-usage-examples/blob/master/mount_s3_bucket_and_run_jupyter_on_ec2.ipynb" class="" target="_none">Exploring ENCODE data from EC2 with Jupyter notebook</a> by <a href="https://github.com/keenangraham" target="_none">Keenan Graham</a></li></a>
          </li>
          <li>
            <a href="https://github.com/ENCODE-DCC/encode-data-usage-examples/blob/master/ctcf_chip_seq_cross_cell_type_correlation.ipynb" class="" target="_none">ENCODE CTCF ChIP-seq data correlation across different cell types</a> by <a href="https://github.com/p-sud" target="_none">Paul Sud</a></li></a>
          </li>
          <li>
            <a href="https://academic.oup.com/nar/article/48/D1/D882/5622708#190992715" class="" target="_none">New developments on the Encyclopedia of DNA Elements (ENCODE) data portal</a> by <a href="http://orcid.org/0000-0001-6948-2042" target="_none">Luo et al 2020</a></li></a>
          </li>
          <li>
            <a href="https://github.com/ENCODE-DCC/encode-data-usage-examples/blob/master/ingest_encode_data_tile_db_with_s3_backend.ipynb" class="" target="_none">Ingesting ENCODE data into TileDB with S3 backend</a> by <a href="https://github.com/ottojolanki" target="_none">Otto Jolanki</a></li></a>
          </li>
        </ul>
        <p><a href="/encode-project/#usageexamples">See 4 usage examples &rarr;</a></p>
        </div>
        <div id="allen-mouse-brain-atlas" class="dataset">
          <h3><a href="/allen-mouse-brain-atlas/">Allen Mouse Brain Atlas</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">gene expression</span><span class="label label-info tag link-tag">genetic</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">Mus musculus</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">transcriptomics</span></p>
          <p>The Allen Mouse Brain Atlas is a genome-scale collection of cellular resolution gene expression profiles using in situ hybridization (ISH). Highly methodical data production methods and comprehensive anatomical coverage via dense, uniformly spaced sampling facilitate data consistency and comparability across &gt;20,000 genes. The use of an inbred mouse strain with minimal animal-to-animal variance allows one to treat the brain essentially as a complex but highly reproducible three-dimensional tissue array. The entire Allen Mouse Brain Atlas dataset and associated tools are available through an...</p>
          <p><a href="/allen-mouse-brain-atlas/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/AllenInstitute/open_dataset_tools/blob/master/Visualizing_Images_from_Allen_Mouse_Brain_Atlas.ipynb" class="" target="_none">Visualizing Images from the Allen Mouse Brain Atlas</a> by <a href="www.alleninstitute.org" target="_none">Allen Institute for Brain Science</a></li></a>
          </li>
          <li>
            <a href="http://www.nature.com/articles/nature05453" class="" target="_none">Genome-wide atlas of gene expression in the adult mouse brain</a> by <a href="www.alleninstitute.org" target="_none">Ed Lein, et al.</a></li></a>
          </li>
          <li>
            <a href="https://mouse.brain-map.org" class="" target="_none">Allen Mouse Brain Atlas</a> by <a href="www.alleninstitute.org" target="_none">Allen Institute for Brain Science</a></li></a>
          </li>
        </ul>
        <p><a href="/allen-mouse-brain-atlas/#usageexamples">See 3 usage examples &rarr;</a></p>
        </div>
        <div id="grillo-openeew" class="dataset">
          <h3><a href="/grillo-openeew/">OpenEEW</a></h3>
          <p><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">earthquakes</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>Grillo has developed an IoT-based earthquake early-warning system,
with sensors currently deployed in Mexico, Chile, Puerto Rico and Costa Rica,
and is now opening its entire archive of unprocessed accelerometer
data to the world to encourage the development of new algorithms
capable of rapidly detecting and characterizing earthquakes in
real time.</p>
          <p><a href="/grillo-openeew/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://openeew.com/docs/analyze-historic" class="" target="_none">Analyzing a magnitude 7.2 earthquake in Mexico using Python</a> by <a href="https://grillo.io/" target="_none">Grillo</a></li></a>
          </li>
          <li>
            <a href="https://github.com/openeew/openeew-python" class="" target="_none">OpenEEW library for Python</a> by <a href="https://grillo.io/" target="_none">Grillo</a></li></a>
          </li>
          <li>
            <a href="https://openeew.com/docs/machine-learning" class="" target="_none">Developing a machine learning model for better earthquake detection</a> by <a href="https://grillo.io/" target="_none">Grillo</a></li></a>
          </li>
        </ul>
        <p><a href="/grillo-openeew/#usageexamples">See 3 usage examples &rarr;</a></p>
        </div>
        <div id="sorel-20m" class="dataset">
          <h3><a href="/sorel-20m/">Sophos/ReversingLabs 20 Million malware detection dataset</a></h3>
          <p><span class="label label-info tag link-tag">cyber security</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">labeled</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>A dataset intended to support research on machine learning
techniques for detecting malware.  It includes metadata and EMBER-v2
features for approximately 10 million benign and 10 million malicous
Portable Executable files, with disarmed but otherwise complete
files for all malware samples.  All samples are labeled using Sophos
in-house labeling methods, have features extracted using the
EMBER-v2 feature set, well as metadata obtained via the pefile
python library, detection counts obtained via ReversingLabs
telemetry, and additional behavioral tags that indicate the rough
behavior of the samp...</p>
          <p><a href="/sorel-20m/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/sophos-ai/SOREL-20M" class="" target="_none">SOREL-20M dataset interface code</a> by Richard Harang and Ethan M Rudd</li></a>
          </li>
          <li>
            <a href="https://github.com/sophos-ai/SOREL-20M#quickstart" class="" target="_none">SOREL-20M quickstart</a> by Richard Harang</li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2012.07634" class="" target="_none">SOREL-20M: A Large Scale Benchmark Dataset for Malicious PE Detection
</a> by Richard Harang and Ethan M Rudd</li></a>
          </li>
        </ul>
        <p><a href="/sorel-20m/#usageexamples">See 3 usage examples &rarr;</a></p>
        </div>
        <div id="afsis" class="dataset">
          <h3><a href="/afsis/">Africa Soil Information Service (AfSIS) Soil Chemistry</a></h3>
          <p><span class="label label-info tag link-tag">agriculture</span><span class="label label-info tag link-tag">environmental</span><span class="label label-info tag link-tag">food security</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>This dataset contains soil infrared spectral data and paired soil property
reference measurements for georeferenced soil samples that were collected
through the Africa Soil Information Service (AfSIS) project, which lasted
from 2009 through 2018. In this release, we include data collected during
Phase I (2009-2013.) Georeferenced samples were collected from 19 countries
in Sub-Saharan African using a statistically sound sampling scheme,
and their soil properties were analyzed using <em>both</em> conventional soil
testing methods and spectral methods (infrared diffuse reflectance
spectroscopy). The two ...</p>
          <p><a href="/afsis/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/qedsoftware/afsis-soil-chem-tutorial/" class="" target="_none">AfSIS Soil Chemistry - Usage Tutorial</a> by <a href="https://qed.ai" target="_none">QED</a></li></a>
          </li>
          <li>
            <a href="https://www.youtube.com/watch?v&#x3D;Fb9R0CnPMkc" class="" target="_none">Goalkeepers 2018, Soil - The Big Data Beneath Your Feet</a> by <a href="https://qed.ai" target="_none">QED</a></li></a>
          </li>
        </ul>
        <p><a href="/afsis/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="amazon-bin-imagery" class="dataset">
          <h3><a href="/amazon-bin-imagery/">Amazon Bin Image Dataset</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>The Amazon Bin Image Dataset contains over 500,000 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations.</p>
          <p><a href="/amazon-bin-imagery/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/silverbottlep/abid_challenge" class="" target="_none">Amazon Bin Image Dataset Challenge</a> by <a href="https://github.com/silverbottlep" target="_none">silverbottlep</a></li></a>
          </li>
          <li>
            <a href="https://github.com/OneNow/AI-Inventory-Reconciliation" class="" target="_none">Amazon Inventory Reconciliation using AI</a> by <a href="https://github.com/pablo-tech" target="_none">Pablo Rodriguez Bertorello, Sravan Sripada, Nutchapol Dendumrongsup</a></li></a>
          </li>
        </ul>
        <p><a href="/amazon-bin-imagery/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="meteo-france-models" class="dataset">
          <h3><a href="/meteo-france-models/">Atmospheric Models from Météo-France</a></h3>
          <p><span class="label label-info tag link-tag">agriculture</span><span class="label label-info tag link-tag">climate</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">environmental</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">meteorological</span><span class="label label-info tag link-tag">model</span><span class="label label-info-sustainability tag link-tag">sustainability</span><span class="label label-info tag link-tag">weather</span></p>
          <p>Global and high-resolution regional atmospheric models from Météo-France.<ul>
<li>ARPEGE World covers the entire world at a base horizontal resolution of 0.5° (~55km) between grid points, it predicts weather out up to 114 hours in the future.</li>
<li>ARPEGE Europe covers Europe and North-Africa at a base horizontal resolution of 0.1° (~11km) between grid points, it predicts weather out up to 114 hours in the future.</li>
<li>AROME France covers France at a base horizontal resolution of 0.025° (~2.5km) between grid points, it predicts weather out up to 42 hours in the future.</li>
<li>AROME France HD covers France and neigborhood at a base horizontal resolution of 0.01° (~1.5km) between grid points, it predicts weather out up to 42 hours in the future.</li>
</ul>
Dozens of atmospheric variables are avail...</p>
          <p><a href="/meteo-france-models/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://windguru.cz" class="" target="_none">Windguru.cz</a> by <a href="https://windguru.cz" target="_none">Windguru</a></li></a>
          </li>
          <li>
            <a href="https://windy.com" class="" target="_none">Windy.com</a> by <a href="https://windy.com" target="_none">Windy</a></li></a>
          </li>
        </ul>
        <p><a href="/meteo-france-models/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="janelia-cosem" class="dataset">
          <h3><a href="/janelia-cosem/">Cell Organelle Segmentation in Electron Microscopy (COSEM) on AWS</a></h3>
          <p><span class="label label-info tag link-tag">cell biology</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">electron microscopy</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">organelle</span></p>
          <p>High resolution images of subcellular structures.</p>
          <p><a href="/janelia-cosem/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://elifesciences.org/articles/25916" class="" target="_none">Enhanced FIB-SEM systems for large-volume 3D imaging</a> by C. Shan Xu, Kenneth J. Hayworth, Zhiyuan Lu, Patricia Grob, Ahmed M. Hassan, José G. García-Cerdán, Krishna K. Niyogi, Eva Nogales, Richard J. Weinberg, Harald F. Hess.</li></a>
          </li>
          <li>
            <a href="https://science.sciencemag.org/content/367/6475/eaaz5357" class="" target="_none">Correlative three-dimensional super-resolution and block-face electron microscopy of whole vitreously frozen cells.</a> by David P. Hoffman1, Gleb Shtengel, C. Shan Xu, Kirby R. Campbell, Melanie Freeman, Lei Wang, Daniel E. Milkie, H. Amalia Pasolli, Nirmala Iyer, John A. Bogovic, Daniel R. Stabley, Abbas Shirinifard, Song Pang, David Peale, Kathy Schaefer, Wim Pomp, Chi-Lun Chang, Jennifer Lippincott-Schwartz, Tom Kirchhausen1, David J. Solecki, Eric Betzig, Harald F. Hess.</li></a>
          </li>
        </ul>
        <p><a href="/janelia-cosem/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="digitalcorpora" class="dataset">
          <h3><a href="/digitalcorpora/">DigitalCorpora</a></h3>
          <p><span class="label label-info tag link-tag">computer forensics</span><span class="label label-info tag link-tag">computer security</span><span class="label label-info tag link-tag">CSI</span><span class="label label-info tag link-tag">cyber security</span><span class="label label-info tag link-tag">digital forensics</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">information retrieval</span><span class="label label-info tag link-tag">internet</span><span class="label label-info tag link-tag">intrusion detection</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">machine translation</span><span class="label label-info tag link-tag">text analysis</span></p>
          <p>Disk images, memory dumps, network packet captures, and files for use in digital forensics research and education. All of this information is accessible through the digitalcorpora.org website, and made available at s3://digitalcorpora/. Some of these datasets implement scenarios that were performed by students, faculty, and others acting <em>in persona</em>. As such, the information is synthetic and may be used without prior authorization or IRB approval. Details of these datasets can be found at <a href="http://www.simson.net/clips/academic/2009...</p>
          <p><a href="/digitalcorpora/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="http://www.simson.net/clips/academic/2009.DFRWS.Corpora.pdf" class="" target="_none">Bringing Science to Digital Forensics with Standardized Forensic Corpora</a> by <a href="https://simson.net/" target="_none">Garfinkel, Farrell, Roussev and Dinolt</a></li></a>
          </li>
          <li>
            <a href="http://simson.net/clips/academic/2011.ADFSL.Corpora.pdf" class="" target="_none">Creating Realistic Corpora for Forensic and Security Education</a> by <a href="https://simson.net/" target="_none">Woods, K., Christopher Lee, Simson Garfinkel, David Dittrich, Adam Russel, Kris Kearton</a></li></a>
          </li>
        </ul>
        <p><a href="/digitalcorpora/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="rareplanes" class="dataset">
          <h3><a href="/rareplanes/">RarePlanes</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">labeled</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">satellite imagery</span></p>
          <p>RarePlanes is a unique open-source machine learning dataset from CosmiQ Works and AI.Reverie that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of AI.Reverie synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very high resolution dataset built to test the value of synthetic data from an overhead perspective. The real portion ...</p>
          <p><a href="/rareplanes/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/2006.02963" class="" target="_none">RarePlanes: Synthetic Data Takes Flight</a> by Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan Lewis, Daeil Kim</li></a>
          </li>
          <li>
            <a href="https://github.com/aireveries/RarePlanes" class="" target="_none">RarePlanes Codebase</a> by Thomas Hossler and Jacob Shermeyer</li></a>
          </li>
        </ul>
        <p><a href="/rareplanes/#usageexamples">See 2 usage examples &rarr;</a></p>
        </div>
        <div id="allen-brain-observatory" class="dataset">
          <h3><a href="/allen-brain-observatory/">Allen Brain Observatory - Visual Coding AWS Public Data Set</a></h3>
          <p><span class="label label-info tag link-tag">electrophysiology</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">Mus musculus</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">signal processing</span></p>
          <p>The Allen Brain Observatory – Visual Coding is a large-scale, standardized survey of physiological activity across the mouse visual cortex, hippocampus, and thalamus. It includes datasets collected with both two-photon imaging and Neuropixels probes, two complementary techniques for measuring the activity of neurons in vivo. The two-photon imaging dataset features visually evoked calcium responses from GCaMP6-expressing neurons in a range of cortical layers, visual areas, and Cre lines. The Neuropixels dataset features spiking activity from distributed cortical and subcortical brain regions, c...</p>
          <p><a href="/allen-brain-observatory/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/AllenInstitute/AllenSDK/wiki/Use-the-Allen-Brain-Observatory-%E2%80%93-Visual-Coding-on-AWS" class="" target="_none">Use the Allen Brain Observatory – Visual Coding on AWS</a> by <a href="https://twitter.com/AllenInstitute" target="_none">Nika Keller, David Feng</a></li></a>
          </li>
        </ul>
        <p><a href="/allen-brain-observatory/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="answer-reformulation" class="dataset">
          <h3><a href="/answer-reformulation/">Answer Reformulation</a></h3>
          <p><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>Original StackExchange answers and their voice-friendly Reformulation.</p>
          <p><a href="/answer-reformulation/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://www2020.thewebconf.org/" class="" target="_none">Voice-based Reformulation of Community Answers</a> by Simone Filice, Nachshon Cohen &amp; David Carmel</li></a>
          </li>
        </ul>
        <p><a href="/answer-reformulation/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="asr-error-robustness" class="dataset">
          <h3><a href="/asr-error-robustness/">Automatic Speech Recognition (ASR) Error Robustness</a></h3>
          <p><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span><span class="label label-info tag link-tag">speech recognition</span></p>
          <p>Sentence classification datatasets with ASR Errors.</p>
          <p><a href="/asr-error-robustness/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://assets.amazon.science/64/94/639ae0c44890837b0f1fbf11ef77/using-phoneme-representations-to-build-predictive-models-robust-to-asr-errors.pdf" class="" target="_none">Using Phoneme Representations to Build Predictive Models Robust to ASR Errors</a> by <a href="https://anjiefang.github.io" target="_none">Anjie Fang, Simone Filice, Nut Limsopatham and Oleg Rokhlenko</a></li></a>
          </li>
        </ul>
        <p><a href="/asr-error-robustness/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="dialoglue" class="dataset">
          <h3><a href="/dialoglue/">DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue</a></h3>
          <p><span class="label label-info tag link-tag">conversation data</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>This bucket contains the checkpoints used to reproduce the baseline results reported in the DialoGLUE benchmark hosted
on EvalAI (<a href="https://evalai.cloudcv.org/web/challenges/challenge-page/708/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/708/overview</a>). The associated scripts for using the checkpoints are located here:
<a href="https://github.com/alexa/dialoglue">https://github.com/alexa/dialoglue</a>. The associated paper describing the benchmark and checkpoints is here: <a href="https://arxiv.org/abs/2009.13570">https://arxiv.org/abs/2009.13570</a>.
The provided checkpoints include the CONVBERT model, a BERT-esque model trained on a large open-domain conversational
dataset. It also includes the CONVBERT-DG and BERT-DG checkpoints descri...</p>
          <p><a href="/dialoglue/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/2009.13570" class="" target="_none">DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue</a> by Shikib Mehri, Mihail Eric, Dilek Hakkani-Tur</li></a>
          </li>
        </ul>
        <p><a href="/dialoglue/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="topical-chat-enriched" class="dataset">
          <h3><a href="/topical-chat-enriched/">Enriched Topical-Chat Dataset for Knowledge-Grounded Dialogue Systems</a></h3>
          <p><span class="label label-info tag link-tag">conversation data</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>This dataset provides extra annotations on top of the publicly released
Topical-Chat dataset(<a href="https://github.com/alexa/Topical-Chat">https://github.com/alexa/Topical-Chat</a>) which will help in reproducing the results in our paper
&quot;Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems&quot; (<a href="https://arxiv.org/abs/2005.12529?context=cs.CL">https://arxiv.org/abs/2005.12529?context=cs.CL</a>). 
The dataset contains 5 files: train.json, valid_freq.json, valid_rare.json, test_freq.json and test_rare.json. 
Each of these files will have additional annotations on top of the original Topical-Chat dataset.
These specific annotations are: dialogue act annotations a...</p>
          <p><a href="/topical-chat-enriched/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/2005.12529?context&#x3D;cs.CL" class="" target="_none">Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems</a> by Behnam Hedayatnia, Karthik Gopalakrishnan, Seokhwan Kim, Yang Liu, Mihail Eric &amp; Dilek Hakkani-Tur</li></a>
          </li>
        </ul>
        <p><a href="/topical-chat-enriched/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="humor-detection" class="dataset">
          <h3><a href="/humor-detection/">Humor Detection from Product Question Answering Systems</a></h3>
          <p><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>This dataset provides labeled humor detection from product question answering systems.
The dataset contains 3 csv files: <a href="https://humor-detection-pds.s3-us-west-2.amazonaws.com/Humorous.csv">Humorous.csv</a> 
containing the humorous product questions, 
<a href="https://humor-detection-pds.s3-us-west-2.amazonaws.com/Non-humorous-unbiased.csv">Non-humorous-unbiased.csv</a> containing 
the non-humorous prodcut questions from the same products as the humorous one, and, 
<a href="https://humor-detection-pds.s3-us-west-2.a...</p>
          <p><a href="/humor-detection/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://sigir.org/sigir2020/" class="" target="_none">Humor Detection in Product Question Answering Systems.</a> by Yftah Ziser, Elad Kravi &amp; David Carmel</li></a>
          </li>
        </ul>
        <p><a href="/humor-detection/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="ocmr_data" class="dataset">
          <h3><a href="/ocmr_data/">Ohio State Cardiac MRI Raw Data (OCMR)</a></h3>
          <p><span class="label label-info tag link-tag">Homo sapiens</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">signal processing</span></p>
          <p>OCMR is an open-access repository that provides multi-coil k-space data for cardiac cine.  The fully sampled MRI datasets are intended for quantitative comparison and evaluation of image reconstruction methods. The free-breathing, prospectively undersampled datasets are intended to evaluate their performance and generalizability qualitatively.</p>
          <p><a href="/ocmr_data/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/MRIOSU/OCMR/blob/master/Python/example_ocmr.ipynb" class="" target="_none">OCMR Tutorial</a> by <a href="https://cmr.engineering.osu.edu/people/trainees" target="_none">Chong Chen</a></li></a>
          </li>
        </ul>
        <p><a href="/ocmr_data/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="qiime2" class="dataset">
          <h3><a href="/qiime2/">QIIME 2 User Tutorial Datasets</a></h3>
          <p><span class="label label-info tag link-tag">bioinformatics</span><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">denoising</span><span class="label label-info tag link-tag">ecosystems</span><span class="label label-info tag link-tag">environmental</span><span class="label label-info tag link-tag">genetic</span><span class="label label-info tag link-tag">genomic</span><span class="label label-info tag link-tag">health</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">microbiome</span><span class="label label-info tag link-tag">statistics</span></p>
          <p>QIIME 2 is a powerful, extensible, and decentralized microbiome analysis package with a focus on data and analysis transparency. QIIME 2 enables researchers to start an analysis with raw DNA sequence data and finish with publication-quality figures and statistical results. This dataset contains the user docs (and related datasets) for QIIME 2.</p>
          <p><a href="/qiime2/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://docs.qiime2.org/2018.11/install/virtual/aws/" class="" target="_none">Installing QIIME 2 using Amazon Web Services</a> by <a href="https://github.com/qiime2/docs/graphs/contributors" target="_none">The QIIME 2 Development Team</a></li></a>
          </li>
        </ul>
        <p><a href="/qiime2/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="sentinel-s2-l2a-mosaic-120" class="dataset">
          <h3><a href="/sentinel-s2-l2a-mosaic-120/">Sentinel-2 L2A 120m Mosaic</a></h3>
          <p><span class="label label-info tag link-tag">agriculture</span><span class="label label-info tag link-tag">earth observation</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural resource</span><span class="label label-info tag link-tag">satellite imagery</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>Sentinel-2 L2A 120m mosaic is a derived product, which contains best pixel values for 10-daily periods, modelled by removing the cloudy pixels and then performing interpolation among remaining values. As there are some parts of the world, which have lengthy cloudy periods, clouds might be remaining in some parts. The actual modelling script is available <a href="https://sentinel-hub.github.io/custom-scripts/sentinel-2/interpolated_time_series/">here</a>.</p>
          <p><a href="/sentinel-s2-l2a-mosaic-120/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/sentinel-hub/public-collections/blob/main/collections/sentinel-s2-l2a-mosaic-120.md" class="" target="_none">Sentinel Hub WMS/WMTS/WCS Service and Process API</a> by <a href="http://www.sinergise.com/" target="_none">Sinergise</a></li></a>
          </li>
        </ul>
        <p><a href="/sentinel-s2-l2a-mosaic-120/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="tabula-muris" class="dataset">
          <h3><a href="/tabula-muris/">Tabula Muris</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">encyclopedic</span><span class="label label-info tag link-tag">genomic</span><span class="label label-info tag link-tag">health</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">medicine</span></p>
          <p>Tabula Muris is a compendium of single cell transcriptomic data from the model organism <em>Mus musculus</em> comprising more than 100,000 cells from 20 organs and tissues. These data represent a new resource for cell biology, reveal gene expression in poorly characterized cell populations, and allow for direct and controlled comparison of gene expression in cell types shared between tissues, such as T-lymphocytes and endothelial cells from different anatomical locations. Two distinct technical approaches were used for most organs: one approach, microfluidic droplet-based 3’-end counting, enabled the s...</p>
          <p><a href="/tabula-muris/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/30283141" class="" target="_none">Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris.</a> by <a href="https://tabula-muris.ds.czbiohub.org/" target="_none">Tabula Muris Consortium (2019)</a></li></a>
          </li>
        </ul>
        <p><a href="/tabula-muris/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="amazon-reviews-ml" class="dataset">
          <h3><a href="/amazon-reviews-ml/">The Multilingual Amazon Reviews Corpus</a></h3>
          <p><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>We present a collection of Amazon reviews specifically designed to aid research in multilingual text classification. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID and the coarse-grained product category (e.g. &#39;books&#39;, &#39;appliances&#39;, etc.)</p>
          <p><a href="/amazon-reviews-ml/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/2010.02573" class="" target="_none">The Multilingual Amazon Reviews Corpus</a> by Phillip Keung, Yichao Lu, György Szarvas, Noah A. Smith</li></a>
          </li>
        </ul>
        <p><a href="/amazon-reviews-ml/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="lab41-sri-voices" class="dataset">
          <h3><a href="/lab41-sri-voices/">Voices Obscured in Complex Environmental Settings (VOiCES)</a></h3>
          <p><span class="label label-info tag link-tag">automatic speech recognition</span><span class="label label-info tag link-tag">denoising</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">speaker identification</span><span class="label label-info tag link-tag">speech processing</span></p>
          <p>VOiCES is a speech corpus recorded in acoustically challenging settings,
using distant microphone recording. Speech was recorded in real rooms with various
acoustic features (reverb, echo, HVAC systems, outside noise, etc.). Adversarial noise,
either television, music, or babble, was concurrently played with clean speech.
Data was recorded using multiple microphones strategically placed
throughout the room. The corpus includes audio recordings, orthographic transcriptions,
and speaker labels.</p>
          <p><a href="/lab41-sri-voices/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/voices18/utilities/blob/master/Using%20VOiCES%20corpus%20tutorial.ipynb" class="" target="_none">Getting started with VOiCES data</a> by M.A. Barrios</li></a>
          </li>
        </ul>
        <p><a href="/lab41-sri-voices/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="xiph-media" class="dataset">
          <h3><a href="/xiph-media/">Xiph.Org Test Media</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">media</span><span class="label label-info tag link-tag">movies</span><span class="label label-info tag link-tag">multimedia</span></p>
          <p>Uncompressed video used for video compression and video processing research.</p>
          <p><a href="/xiph-media/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://media.xiph.org/aws.html" class="" target="_none">Encoding video with AV1 on EC2</a> by <a href="https://www.xiph.org/" target="_none">Thomas Daede</a></li></a>
          </li>
        </ul>
        <p><a href="/xiph-media/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="aev-a2d2" class="dataset">
          <h3><a href="/aev-a2d2/">A2D2: Audi Autonomous Driving Dataset</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">lidar</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">mapping</span><span class="label label-info tag link-tag">robotics</span></p>
          <p>An open multi-sensor dataset for autonomous driving research. This dataset comprises semantically segmented images, semantic point clouds, and 3D bounding boxes. In addition, it contains unlabelled 360 degree camera images, lidar, and bus data for three sequences. We hope this dataset will further facilitate active research and development in AI, computer vision, and robotics for autonomous driving.</p>
          <p><a href="/aev-a2d2/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-coco" class="dataset">
          <h3><a href="/fast-ai-coco/">COCO - Common Objects in Context - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>COCO is a large-scale object detection, segmentation, and captioning dataset.
This is part of the fast.ai datasets collection hosted by AWS for convenience
of fast.ai students. If you use this dataset in your research please cite
arXiv:1405.0312 [cs.CV].</p>
          <p><a href="/fast-ai-coco/">Details &rarr;</a></p>
        </div>
        <div id="dataforgood-fb-hrsl" class="dataset">
          <h3><a href="/dataforgood-fb-hrsl/">High Resolution Population Density Maps + Demographic Estimates by CIESIN and Facebook</a></h3>
          <p><span class="label label-info tag link-tag">aerial imagery</span><span class="label label-info tag link-tag">demographics</span><span class="label label-info tag link-tag">disaster response</span><span class="label label-info tag link-tag">geospatial</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">population</span><span class="label label-info tag link-tag">satellite imagery</span><span class="label label-info-sustainability tag link-tag">sustainability</span></p>
          <p>Population data for a selection of countries, allocated to 1 arcsecond blocks and provided in a combination of CSV
and Cloud-optimized GeoTIFF files. This refines <a href="https://sedac.ciesin.columbia.edu/data/collection/gpw-v4">CIESIN’s Gridded Population of the World</a>
using machine learning models on high-resolution worldwide Digital Globe
satellite imagery. CIESIN population counts aggregated from worldwide census
data are allocated to blocks where imagery appears to contain buildings.</p>
          <p><a href="/dataforgood-fb-hrsl/">Details &rarr;</a></p>
        </div>
        <div id="ichangemycity" class="dataset">
          <h3><a href="/ichangemycity/">IChangeMyCity Complaints Data from Janaagraha</a></h3>
          <p><span class="label label-info tag link-tag">cities</span><span class="label label-info tag link-tag">civic</span><span class="label label-info tag link-tag">complaints</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>The <a href="https://www.ichangemycity.com">IChangeMyCity</a> project provides insight into the complaints raised by citizens from diffent cities of India related to the issues in their neighbourhoods and the resolution of the same by the civic bodies.</p>
          <p><a href="/ichangemycity/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-imageclas" class="dataset">
          <h3><a href="/fast-ai-imageclas/">Image classification - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>Some of the most important datasets for image classification research, including
CIFAR 10 and 100, Caltech 101, MNIST, Food-101, Oxford-102-Flowers, Oxford-IIIT-Pets,
and Stanford-Cars.  This is part of the fast.ai datasets collection hosted by
AWS for convenience of fast.ai students. See documentation link for citation and
license details for each dataset.</p>
          <p><a href="/fast-ai-imageclas/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-imagelocal" class="dataset">
          <h3><a href="/fast-ai-imagelocal/">Image localization  - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span></p>
          <p>Some of the most important datasets for image localization  research, including
Camvid and PASCAL VOC (2007 and 2012). This is part of the fast.ai datasets
collection hosted by AWS for convenience of fast.ai students. See
documentation link for citation and license details for each dataset.</p>
          <p><a href="/fast-ai-imagelocal/">Details &rarr;</a></p>
        </div>
        <div id="kitti" class="dataset">
          <h3><a href="/kitti/">KITTI Vision Benchmark Suite</a></h3>
          <p><span class="label label-info tag link-tag">autonomous vehicles</span><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">robotics</span></p>
          <p>Dataset and benchmarks for computer vision research in the context of autonomous driving. The dataset has been recorded in and around the city of Karlsruhe, Germany using the mobile platform AnnieWay (VW station wagon) which has been equipped with several RGB and monochrome cameras, a Velodyne HDL 64 laser scanner as well as an accurate RTK corrected GPS/IMU localization unit. The dataset has been created for computer vision and machine learning research on stereo, optical flow, visual odometry, semantic segmentation, semantic instance segmentation, road segmentation, single image depth predic...</p>
          <p><a href="/kitti/">Details &rarr;</a></p>
        </div>
        <div id="multimedia-commons" class="dataset">
          <h3><a href="/multimedia-commons/">Multimedia Commons</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">multimedia</span></p>
          <p>The Multimedia Commons is a collection of audio and visual features computed for the nearly 100 million Creative Commons-licensed Flickr images and videos in the YFCC100M dataset from Yahoo! Labs, along with ground-truth annotations for selected subsets. The International Computer Science Institute (ICSI) and Lawrence Livermore National Laboratory are producing and distributing a core set of derived feature sets and annotations as part of an effort to enable large-scale video search capabilities. They have released this feature corpus into the public domain, under Creative Commons License 0, s...</p>
          <p><a href="/multimedia-commons/">Details &rarr;</a></p>
        </div>
        <div id="fast-ai-nlp" class="dataset">
          <h3><a href="/fast-ai-nlp/">NLP - fast.ai datasets</a></h3>
          <p><span class="label label-info tag link-tag">deep learning</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>Some of the most important datasets for NLP, with a focus on classification, including
IMDb, AG-News, Amazon Reviews (polarity and full), Yelp Reviews (polarity and
full), Dbpedia, Sogou News (Pinyin), Yahoo Answers, Wikitext 2 and Wikitext
103, and ACL-2010 French-English 10^9 corpus.  This is part of the
fast.ai datasets collection hosted by AWS for convenience of fast.ai
students. See documentation link for citation and license details for each
dataset.</p>
          <p><a href="/fast-ai-nlp/">Details &rarr;</a></p>
        </div>
        <div id="nsd" class="dataset">
          <h3><a href="/nsd/">Natural Scenes Dataset</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span><span class="label label-info tag link-tag">nifti</span></p>
          <p>Here, we collected and pre-processed a massive, high-quality 7T fMRI dataset that can be used to advance our understanding of how the brain works. A unique feature of this dataset is the massive amount of data available per individual subject. The data were acquired using ultra-high-field fMRI (7T, whole-brain, 1.8-mm resolution, 1.6-s TR). We measured fMRI responses while each of 8 participants viewed 9,000–10,000 distinct, color natural scenes (22,500–30,000 trials) in 30–40 weekly scan sessions over the course of a year. Additional measures were collected including resting-state data, retin...</p>
          <p><a href="/nsd/">Details &rarr;</a></p>
        </div>
        <div id="tabula-muris-senis" class="dataset">
          <h3><a href="/tabula-muris-senis/">Tabula Muris Senis</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">encyclopedic</span><span class="label label-info tag link-tag">genomic</span><span class="label label-info tag link-tag">health</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">medicine</span><span class="label label-info tag link-tag">single-cell transcriptomics</span></p>
          <p>Tabula Muris Senis is a comprehensive compendium of single cell transcriptomic data from the model organism <em>Mus musculus</em> comprising more than 500,000 cells from 18 organs and tissues across the mouse lifespan. We discovered cell-specific changes occurring across multiple cell types and organs, as well as age related changes in the cellular composition of different organs. Using single-cell transcriptomic data we were able to assess cell type specific manifestations of different hallmarks of aging, such as senescence, changes in the activity of metabolic pathways, depletion of stem-cell populat...</p>
          <p><a href="/tabula-muris-senis/">Details &rarr;</a></p>
        </div>
        <div id="mmid" class="dataset">
          <h3><a href="/mmid/">The Massively Multilingual Image Dataset (MMID)</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">machine translation</span><span class="label label-info tag link-tag">natural language processing</span></p>
          <p>MMID is a large-scale, massively multilingual dataset of images paired with the words they represent collected at the <a href="https://upenn.edu">University of Pennsylvania</a>.
The dataset is doubly parallel: for each language, words are stored parallel to images that represent the word, <em>and</em> parallel to the word&#39;s translation into English (and corresponding images.)</p>
          <p><a href="/mmid/">Details &rarr;</a></p>
        </div>
      </div>
    </div>


    <hr/>
  </div>  </body>
  <script src="/js/index.js"></script>
</html>
