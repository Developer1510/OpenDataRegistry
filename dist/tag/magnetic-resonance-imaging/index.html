<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
  
    
  
    <link rel="alternate" type="application/rss+xml" href="/rss.xml" />
    
    <link rel="shortcut icon" type="image/ico" href="https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico"/>
    <link rel="apple-touch-icon" sizes="57x57" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="72x72" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>
    <link rel="apple-touch-icon" sizes="114x114" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-iphone-114-smile.png"/>
    <link rel="apple-touch-icon" sizes="144x144" href="https://a0.awsstatic.com/libra-css/images/site/touch-icon-ipad-144-smile.png"/>  
  
    <title>Registry of Open Data on AWS</title>
  
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://assets.opendata.aws/css/bootstrap/3.4.1/bootstrap.min.css">
  
    <!-- Our local CSS -->
    <link rel="stylesheet" href="/css/main.css">
  
  
  </head>
  <body>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://assets.opendata.aws/js/jquery/3.5.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://assets.opendata.aws/js/bootstrap/3.4.1/bootstrap.min.js"></script>
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="roda-header">
          <h2><a href="/" alt="Home">Registry of Open Data on AWS</a></h2>
          <a href="http://aws.amazon.com/what-is-cloud-computing">
            <img src="https://assets.opendata.aws/img/AWS-Logo_White-Color_300x180.png" alt="Powered by AWS Cloud Computing" id="aws_header_logo">
          </a>
        </div>
      </div>
    </nav>
    <div class="container" >

    <div class="about col-md-5">
      <div class="row aboutbox">
            <h3>About</h3>
            <p>This registry exists to help people discover and share datasets that are available via AWS resources. <a href="https://opendata.aws">Learn more about sharing data on AWS</a>.</p>
    
            <p>See <a href="/tag/magnetic-resonance-imaging/usage-examples">all usage examples for datasets listed in this registry</a> tagged with <strong>magnetic resonance imaging</strong>.</p>
    
        <hr>
    
        <h4>Search datasets (currently <span id="count-matching">13</span> matching <span id="count-matching-text">datasets</span>)</h4>
        <form>
          <div class="form-group">
            <input type="text" id="search-box" class="form-control" placeholder="Search datasets" spellcheck="false" autocorrect="off">
          </div>
        </form>
        <p>You are currently viewing a subset of data tagged with <strong>magnetic resonance imaging</strong>.</p>
    
        <hr>
    
        <h4>Add to this registry</h4>
        <p>If you want to add a dataset or example of how to use a dataset to this registry, please follow the instructions on the <a href="https://github.com/awslabs/open-data-registry/">Registry of Open Data on AWS GitHub repository</a>.</p>
    
        <p>Unless specifically stated in the applicable dataset documentation, datasets available through the Registry of Open Data on AWS are not provided and maintained by AWS. Datasets are provided and maintained by a variety of third parties under a variety of licenses. Please check dataset licenses and related documentation to determine if a dataset may be used for your application.</p>
      </div>
    </div>
    <div class="col-md-6 col-md-offset-1 datasets">
      <div class="row">
        <div id="fcp-indi" class="dataset">
          <h3><a href="/fcp-indi/">International Neuroimaging Data-Sharing Initiative (INDI)</a></h3>
          <p><span class="label label-info tag link-tag">Homo sapiens</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span></p>
          <p>This bucket contains multiple neuroimaging datasets that are part of the International Neuroimaging Data-Sharing Initiative. Raw human and non-human primate neuroimaging data include 1) Structural MRI; 2) Functional MRI; 3) Diffusion Tensor Imaging; 4) Electroencephalogram (EEG)
In addition to the raw data, preprocessed data is also included for some datasets.
A complete list of the available datasets can be seen in the documentation lonk provided below. </p>
          <p><a href="/fcp-indi/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://www.nature.com/articles/s41467-018-04976-1" class="" target="_none">Assessment of the impact of shared brain imaging data on the scientific literature</a> by M.P. Milham, R.C. Craddock, ..., A. Klein</li></a>
          </li>
          <li>
            <a href="https://fcon_1000.projects.nitrc.org/indi/s3/index.html" class="" target="_none">Downloading FCP-INDI Neuroimaging Data from Amazon S3</a> by INDI</li></a>
          </li>
          <li>
            <a href="https://www.nature.com/articles/mp201378" class="" target="_none">The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism.</a> by A. Di Martino, C-G Yan, ..., M.P. Milham</li></a>
          </li>
          <li>
            <a href="https://www.nature.com/articles/sdata201710" class="" target="_none">Enhancing studies of the connectome in autism using the autism brain imaging data exchange II.</a> by A. Di Martino, D. O&#x27;Connor, M.P. Milham</li></a>
          </li>
          <li>
            <a href="https://www.sciencedirect.com/science/article/pii/S0896627318307682" class="" target="_none">An Open Resource for Non-human Primate Imaging</a> by M.P. Milham, L. Ai, ..., C.E. Schroeder</li></a>
          </li>
        </ul>
        <p><a href="/fcp-indi/#usageexamples">See 11 usage examples &rarr;</a></p>
        </div>
        <div id="open-neurodata" class="dataset">
          <h3><a href="/open-neurodata/">Open NeuroData</a></h3>
          <p><span class="label label-info tag link-tag">array tomography</span><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">electron microscopy</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">lightsheet microscopy</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span></p>
          <p>This bucket contains multiple neuroimaging datasets (as Neuroglancer Precomputed Volumes) across multiple modalities and scales, ranging from nanoscale (electron microscopy), to microscale (cleared lightsheet microscopy and array tomography), and mesoscale (structural and functional magnetic resonance imaging). Additionally, many of the datasets include segmentations and meshes.</p>
          <p><a href="/open-neurodata/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/1306.3543" class="" target="_none">The Open Connectome Project Data Cluster: Scalable Analysis and Vision for High-Throughput Neuroscience</a> by R. Burns, W. G. Roncal, D. Kleissas, K. Lillaney, P. Manavalan, E. Perlman, D. R. Berger, D. D. Bock, K. Chung, L. Grosenick, N. Kasthuri, N. C. Weiler, K. Deisseroth, M. Kazhdan, J. Lichtman, R. C. Reid, S. J. Smith, A. S. Szalay, J. T. Vogelstein, and R. J. Vogelstein.</li></a>
          </li>
          <li>
            <a href="https://github.com/seung-lab/cloud-volume" class="" target="_none">CloudVolume</a> by <a href="https://github.com/william-silversmith" target="_none">William Silversmith</a></li></a>
          </li>
          <li>
            <a href="https://doi.org/10.1016/j.neuron.2014.08.045" class="" target="_none">From cosmos to connectomes: The evolution of data-intensive science</a> by R. Burns, J. T. Vogelstein, and A. S. Szalay</li></a>
          </li>
          <li>
            <a href="https://github.com/seung-lab/igneous" class="" target="_none">Igneous</a> by <a href="https://github.com/william-silversmith" target="_none">William Silversmith</a></li></a>
          </li>
          <li>
            <a href="https://github.com/google/neuroglancer" class="" target="_none">Neuroglancer</a> by <a href="https://github.com/jbms" target="_none">Jeremy Maitin-Shepard</a></li></a>
          </li>
        </ul>
        <p><a href="/open-neurodata/#usageexamples">See 9 usage examples &rarr;</a></p>
        </div>
        <div id="nyu-fastmri" class="dataset">
          <h3><a href="/nyu-fastmri/">NYU Langone &amp; FAIR FastMRI Dataset</a></h3>
          <p><span class="label label-info tag link-tag">biology</span><span class="label label-info tag link-tag">health</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neurobiology</span><span class="label label-info tag link-tag">neuroimaging</span></p>
          <p>This dataset contains deidentified raw k-space data and DICOM image files of over 1,500 knees and 6,970 brains.</p>
          <p><a href="/nyu-fastmri/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/abs/1904.01112" class="" target="_none">Deep Learning Methods for Parallel Magnetic Resonance Image Reconstruction</a> by <a href="https://arxiv.org/search/eess?searchtype&#x3D;author&amp;query&#x3D;Knoll%2C+F" target="_none">Knoll et al (2019)</a></li></a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/fastMRI/blob/master/fastMRI_tutorial.ipynb" class="" target="_none">FastMRI Tutorial (Jupyter Notebook)</a> by <a href="https://scholar.google.com/citations?user&#x3D;LjjLn2YAAAAJ&amp;hl&#x3D;en" target="_none">Tullie Murrell</a></li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/1811.08839" class="" target="_none">fastMRI:An Open Dataset and Benchmarks for Accelerated MRI</a> by <a href="https://arxiv.org/search/cs?searchtype&#x3D;author&amp;query&#x3D;Zbontar%2C+J" target="_none">Zbontar et al (2019)</a></li></a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2001.02518" class="" target="_none">Advancing machine learning for MR image reconstruction with an open competition:Overview of the 2019 fastMRI challenge</a> by <a href="https://arxiv.org/search/eess?searchtype&#x3D;author&amp;query&#x3D;Knoll%2C+F" target="_none">Knoll et al (2020)</a></li></a>
          </li>
          <li>
            <a href="https://medium.com/@khodges42/how-to-get-started-diy-neurohacking-with-the-fastmri-dataset-and-why-you-should-6a02a6bb896e" class="" target="_none">How to Get Started Working with the FastMRI Dataset (and why you should!)</a> by Kevin Hodges</li></a>
          </li>
        </ul>
        <p><a href="/nyu-fastmri/#usageexamples">See 6 usage examples &rarr;</a></p>
        </div>
        <div id="msd" class="dataset">
          <h3><a href="/msd/">Medical Segmentation Decathlon</a></h3>
          <p><span class="label label-info tag link-tag">computed tomography</span><span class="label label-info tag link-tag">health</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">medicine</span><span class="label label-info tag link-tag">nifti</span><span class="label label-info tag link-tag">segmentation</span></p>
          <p>With recent advances in machine learning, semantic segmentation algorithms are becoming increasingly general purpose and translatable to unseen tasks. Many key algorithmic advances in the field of medical imaging are commonly validated on a small number of tasks, limiting our understanding of the generalisability of the proposed contributions. A model which works out-of-the-box on many tasks, in the spirit of AutoML, would have a tremendous impact on healthcare. The field of medical imaging is also missing a fully open source and comprehensive benchmark for general purpose algorithmic validati...</p>
          <p><a href="/msd/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://arxiv.org/pdf/1902.09063.pdf" class="" target="_none">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</a> by Simpson A. L., Antonelli M., Bakas S., Bilello M., Farahana K., van Ginneken B., et al</li></a>
          </li>
          <li>
            <a href="https://github.com/Project-MONAI/tutorials" class="" target="_none">Pytorch-Integrated MSD Data Loader</a> by <a href="https://github.com/Project-MONAI/MONAI" target="_none">MONAI Development Team</a></li></a>
          </li>
          <li>
            <a href="www.monai.io" class="" target="_none">MONAI: Getting Started</a> by <a href="https://monai.io/start.html" target="_none">MONAI Development Team</a></li></a>
          </li>
        </ul>
        <p><a href="/msd/#usageexamples">See 3 usage examples &rarr;</a></p>
        </div>
        <div id="ocmr_data" class="dataset">
          <h3><a href="/ocmr_data/">Ohio State Cardiac MRI Raw Data (OCMR)</a></h3>
          <p><span class="label label-info tag link-tag">Homo sapiens</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">imaging</span><span class="label label-info tag link-tag">life sciences</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">signal processing</span></p>
          <p>OCMR is an open-access repository that provides multi-coil k-space data for cardiac cine.  The fully sampled MRI datasets are intended for quantitative comparison and evaluation of image reconstruction methods. The free-breathing, prospectively undersampled datasets are intended to evaluate their performance and generalizability qualitatively.</p>
          <p><a href="/ocmr_data/">Details &rarr;</a></p>
          <h4>Usage examples</h4>
          <ul class="dataatwork-list">
          <li>
            <a href="https://github.com/MRIOSU/OCMR/blob/master/Python/example_ocmr.ipynb" class="" target="_none">OCMR Tutorial</a> by <a href="https://cmr.engineering.osu.edu/people/trainees" target="_none">Chong Chen</a></li></a>
          </li>
        </ul>
        <p><a href="/ocmr_data/#usageexamples">See 1 usage example &rarr;</a></p>
        </div>
        <div id="nsd" class="dataset">
          <h3><a href="/nsd/">Natural Scenes Dataset</a></h3>
          <p><span class="label label-info tag link-tag">computer vision</span><span class="label label-info tag link-tag">image processing</span><span class="label label-info tag link-tag">machine learning</span><span class="label label-info tag link-tag">magnetic resonance imaging</span><span class="label label-info tag link-tag">neuroimaging</span><span class="label label-info tag link-tag">neuroscience</span><span class="label label-info tag link-tag">nifti</span></p>
          <p>Here, we collected and pre-processed a massive, high-quality 7T fMRI dataset that can be used to advance our understanding of how the brain works. A unique feature of this dataset is the massive amount of data available per individual subject. The data were acquired using ultra-high-field fMRI (7T, whole-brain, 1.8-mm resolution, 1.6-s TR). We measured fMRI responses while each of 8 participants viewed 9,000–10,000 distinct, color natural scenes (22,500–30,000 trials) in 30–40 weekly scan sessions over the course of a year. Additional measures were collected including resting-state data, retin...</p>
          <p><a href="/nsd/">Details &rarr;</a></p>
        </div>
      </div>
    </div>


    <hr/>
  </div>  </body>
  <script src="/js/index.js"></script>
</html>
